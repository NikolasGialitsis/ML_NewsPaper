<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                                               |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/deploying-a-simple-ui-for-python-88e8e7cbbf61"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/0*qq1KYo_jJmLn2hjl"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Deploying a Simple UI for Python</a></p>&nbsp;&nbsp;                                                                                                  |
| <p><a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Hacking Super Intelligence</a></p>&nbsp;&nbsp;                                                                                                    |
| <p><a href="https://towardsdatascience.com/low-effort-machine-learning-tools-9622d7d57135"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*l-_uMYUimj73v-1JlAsXKg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Low Effort Machine Learning Tools</a></p>&nbsp;&nbsp;                                                                                      |
| <p><a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Emerging problems in machine learning: making AI</a></p>&nbsp;&nbsp;                                                     |
| <p><a href="https://towardsdatascience.com/ocr-and-the-wordsearch-solver-ai-515aeb816bdf"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*5UR8Yol9sE5x0j-T2oA_UQ.gif"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">OCR and the WordSearch solver AI</a></p>&nbsp;&nbsp;                                                                                         |
| <p><a href="https://towardsdatascience.com/data-science-articles-that-provided-value-in-2020-70ba3799ed1f"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*5u8a7lENfkaasvU5AFxttA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Data Science Articles That Provided Value In 2020</a></p>&nbsp;&nbsp;                                                       |
| <p><a href="https://towardsdatascience.com/top-7-data-libraries-you-will-absolutely-need-for-your-next-deep-learning-project-ecf735f25c09"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*Y7bTsQgCcnQ25Ht-"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Top 7 Data Libraries You Will Absolutely Need for Your Next Deep Learning Project</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/machine-learning-on-macos-with-an-amd-gpu-and-plaidml-55a46fe94bc0"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*f0G4yk0STR3lDHlBxAoikg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Machine Learning on macOS with an AMD GPU and PlaidML</a></p>&nbsp;&nbsp;                                               |
| <p><a href="https://towardsdatascience.com/the-step-by-step-curriculum-im-using-to-teach-myself-data-science-in-2021-c8eab834a87c"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*oM5qx_LYK3hit2ak"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">The Step-by-Step Curriculum Using to Teach Myself Data Science in 2021</a></p>&nbsp;&nbsp;                    |
| <p><a href="https://towardsdatascience.com/5-data-science-tips-and-tricks-378517c818d6"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*Z8RkgdIFTCW7AoVlmA8PhA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">5 Data Science Tips and Tricks</a></p>&nbsp;&nbsp;                                                                                             |
| <p><a href="https://towardsdatascience.com/how-to-find-seasonality-using-python-73547ebf322"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*xDIuwKb-wRlhAifk.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">How To Find Seasonality Using Python</a></p>&nbsp;&nbsp;                                                                                        |

| title                                                                                          | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| mean-field behaviour of neural tangent kernel for deep neural networks                         | recent work by jacot et al. (2018) has shown that training a neural network of any kind with gradient descent in parameter space is strongly related to kernel gradient descent in function space with respect to the neural tangent kernel (ntk). lee et al. (2019) built on this result by establishing that the output of a neural network trained using gradient descent can be approximated by a linear model for wide networks. in parallel, a recent line of studies (schoenholz et al. 2017; hayou et al. 2019) has suggested that a special initialization, known as the edge of chaos, improves training. in this paper, we bridge the gap between these two concepts by quantifying the impact of the initialization and the activation function on the ntk when the network depth becomes large. in particular, we show that the performance of wide deep neural networks cannot be explained by the ntk regime and we provide experiments illustrating our theoretical results.                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| graphchallenge.org sparse deep neural network performance	doi:10.1109/hpec43674.2020.9286253   | the mit/ieee/amazon graphchallenge.org encourages community approaches to developing new solutions for analyzing graphs and sparse data. sparse ai analytics present unique scalability difficulties. the sparse deep neural network (dnn) challenge draws upon prior challenges from machine learning, high performance computing, and visual analytics to create a challenge that is reflective of emerging sparse ai systems. the sparse dnn challenge is based on a mathematically well-defined dnn inference computation and can be implemented in any programming environment. in 2019 several sparse dnn challenge submissions were received from a wide range of authors and organizations. this paper presents a performance analysis of the best performers of these submissions. these submissions show that their state-of-the-art sparse dnn execution time, $t_{\rm dnn}$, is a strong function of the number of dnn operations performed, $n_{\rm op}$. the sparse dnn challenge provides a clear picture of current sparse dnn systems and underscores the need for new innovations to achieve high performance on very large sparse dnns.                                                                                                                                                                                                                                                                                         |
| statistical inference of assortative community structures	doi:10.1103/physrevresearch.2.043271 | we develop a principled methodology to infer assortative communities in networks based on a nonparametric bayesian formulation of the planted partition model. we show that this approach succeeds in finding statistically significant assortative modules in networks, unlike alternatives such as modularity maximization, which systematically overfits both in artificial as well as in empirical examples. in addition, we show that our method is not subject to a resolution limit, and can uncover an arbitrarily large number of communities, as long as there is statistical evidence for them. our formulation is amenable to model selection procedures, which allow us to compare it to more general approaches based on the stochastic block model, and in this way reveal whether assortativity is in fact the dominating large-scale mixing pattern. we perform this comparison with several empirical networks, and identify numerous cases where the network's assortativity is exaggerated by traditional community detection methods, and we show how a more faithful degree of assortativity can be identified.                                                                                                                                                                                                                                                                                                              |
| towards automated satellite conjunction management with bayesian deep   learning               | after decades of space travel, low earth orbit is a junkyard of discarded rocket bodies, dead satellites, and millions of pieces of debris from collisions and explosions. objects in high enough altitudes do not re-enter and burn up in the atmosphere, but stay in orbit around earth for a long time. with a speed of 28,000 km/h, collisions in these orbits can generate fragments and potentially trigger a cascade of more collisions known as the kessler syndrome. this could pose a planetary challenge, because the phenomenon could escalate to the point of hindering future space operations and damaging satellite infrastructure critical for space and earth science applications. as commercial entities place mega-constellations of satellites in orbit, the burden on operators conducting collision avoidance manoeuvres will increase. for this reason, development of automated tools that predict potential collision events (conjunctions) is critical. we introduce a bayesian deep learning approach to this problem, and develop recurrent neural network architectures (lstms) that work with time series of conjunction data messages (cdms), a standard data format used by the space community. we show that our method can be used to model all cdm features simultaneously, including the time of arrival of future cdms, providing predictions of conjunction event evolution with associated uncertainties. |
| a modern analysis of hutchinson's trace estimator                                              | the paper establishes the new state-of-art in the accuracy analysis of hutchinson's trace estimator. leveraging tools that have not been previously used in this context, particularly hypercontractive inequalities and concentration properties of sub-gamma distributions, we offer an elegant and modular analysis, as well as numerically superior bounds. besides these improvements, this work aims to better popularize the aforementioned techniques within the cs community.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

| title                                                                                                                                                 | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|-------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| relationship between manifold smoothness and adversarial vulnerability   in deep learning with local errors                                           | artificial neural networks can achieve impressive performances, and even outperform humans in some specific tasks. nevertheless, unlike biological brains, the artificial neural networks suffer from tiny perturbations in sensory input, under various kinds of adversarial attacks. it is therefore necessary to study the origin of the adversarial vulnerability. here, we establish a fundamental relationship between geometry of hidden representations (manifold perspective) and the generalization capability of the deep networks. for this purpose, we choose a deep neural network trained by local errors, and then analyze emergent properties of trained networks through the manifold dimensionality, manifold smoothness, and the generalization capability. to explore effects of adversarial examples, we consider independent gaussian noise attacks and fast-gradient-sign-method (fgsm) attacks. our study reveals that a high generalization accuracy requires a relatively fast power-law decay of the eigen-spectrum of hidden representations. under gaussian attacks, the relationship between generalization accuracy and power-law exponent is monotonic, while a non-monotonic behavior is observed for fgsm attacks. our empirical study provides a route towards a final mechanistic interpretation of adversarial vulnerability under adversarial attacks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| is the brain macroscopically linear? a system identification of resting   state dynamics                                                              | a central challenge in the computational modeling of neural dynamics is the trade-off between accuracy and simplicity. at the level of individual neurons, nonlinear dynamics are both experimentally established and essential for neuronal functioning. an implicit assumption has thus formed that an accurate computational model of whole-brain dynamics must also be highly nonlinear, whereas linear models may provide a first-order approximation. here, we provide a rigorous and data-driven investigation of this hypothesis at the level of whole-brain blood-oxygen-level-dependent (bold) and macroscopic field potential dynamics by leveraging the theory of system identification. using functional mri (fmri) and intracranial eeg (ieeg), we model the resting state activity of 700 subjects in the human connectome project (hcp) and 122 subjects from the restoring active memory (ram) project using state-of-the-art linear and nonlinear model families. we assess relative model fit using predictive power, computational complexity, and the extent of residual dynamics unexplained by the model. contrary to our expectations, linear auto-regressive models achieve the best measures across all three metrics, eliminating the trade-off between accuracy and simplicity. to understand and explain this linearity, we highlight four properties of macroscopic neurodynamics which can counteract or mask microscopic nonlinear dynamics: averaging over space, averaging over time, observation noise, and limited data samples. whereas the latter two are technological limitations and can improve in the future, the former two are inherent to aggregated macroscopic brain activity. our results, together with the unparalleled interpretability of linear models, can greatly facilitate our understanding of macroscopic neural dynamics and the principled design of model-based interventions for the treatment of neuropsychiatric disorders. |
| microcircuit synchronization and heavy tailed synaptic weight   distribution in preb\"otzinger complex contribute to generation of breathing   rhythm | the preb\"otzinger complex, the mammalian inspiratory rhythm generator, encodes inspiratory time as motor pattern. spike synchronization throughout this sparsely connected network generates inspiratory bursts albeit with variable latencies after preinspiratory activity onset in each breathing cycle. using preb\"otc rhythmogenic microcircuit minimal models, we examined the variability in probability and latency to burst, mimicking experiments. among various physiologically plausible graphs of 1000 point neurons with experimentally determined neuronal and synaptic parameters, directed erd\h{o}s-r\'enyi graphs best captured the experimentally observed dynamics. mechanistically, preb\"otc (de)synchronization and oscillatory dynamics are regulated by the efferent connectivity of spiking neurons that gates the amplification of modest preinspiratory activity through input convergence. furthermore, to replicate experiments, a lognormal distribution of synaptic weights was necessary to augment the efficacy of convergent coincident inputs. these mechanisms enable exceptionally robust yet flexible preb\"otc attractor dynamics that, we postulate, represent universal temporal-processing and decision-making computational motifs throughout the brain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| the structure of behavioral data                                                                                                                      | for more than a century, scientists have been collecting behavioral data--an increasing fraction of which is now being publicly shared so other researchers can reuse them to replicate, integrate or extend past results. although behavioral data is fundamental to many scientific fields, there is currently no widely adopted standard for formatting, naming, organizing, describing or sharing such data. this lack of standardization is a major bottleneck for scientific progress. not only does it prevent the effective reuse of data, it also affects how behavioral data in general are processed, as non-standard data calls for custom-made data analysis code and prevents the development of efficient tools. to address this problem, we develop the behaverse data model (bdm), a standard for structuring behavioral data. here we focus on major concepts in behavioral data, leaving further details and developments to the project's website (https://behaverse.github.io/data-model/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |