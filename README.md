<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/how-to-present-machine-learning-results-to-non-technical-people-e096cc1b9f76"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/1*t7lx34vYoJ8lAeNU6un7_g.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">How to Present Machine Learning Results to Non-Technical People</a></p>&nbsp;&nbsp;                                   |
| <p><a href="https://towardsdatascience.com/project-modeling-predicting-of-churning-customers-in-r-cb0a846ba94a"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/1*Bpmj9FmK4lu9Z_SN8z1_Kg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Project: Modeling & Predicting of Churning Customers (in R)</a></p>&nbsp;&nbsp;                                                 |
| <p><a href="https://towardsdatascience.com/how-to-utilize-feature-importance-correctly-1f196b061192"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*wNM_N4w1_v3JHmBpVdsFzw.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">How to utilize Feature Importance correctly</a></p>&nbsp;&nbsp;                                                                            |
| <p><a href="https://towardsdatascience.com/underspecification-the-dangerously-underdiscussed-problem-facing-machine-learning-4882292c67c1"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*YqI_5vfZow0Nhkop-AoWrQ.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Underspecification: The Dangerously Underdiscussed Problem Facing Machine Learning</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/recommendation-systems-via-matrix-factorization-28e7d0aa8ca7"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*udXwB_fJRENtWPJR"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Recommendation Systems via Matrix Factorization</a></p>&nbsp;&nbsp;                                                                               |
| <p><a href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*QWgVR_TznQND4LwnHN01Xg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">How does sparse convolution work?</a></p>&nbsp;&nbsp;                                                                                                   |
| <p><a href="https://towardsdatascience.com/instagram-analysis-to-predict-limited-edition-sneakers-resale-price-with-ann-5838cbecfab3"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*3_boWWeMkU8mmq0TRVjTUg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Instagram Analysis to Predict Limited Edition Sneakers Resale Price with ANN</a></p>&nbsp;&nbsp;          |
| <p><a href="https://towardsdatascience.com/optimization-a-notorious-road-to-structured-inefficiency-and-transition-to-combinatorial-9c7f8232b9d0"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*Y-lloWpqrw9UBzd4"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Optimization: A notorious road to Structured Inefficiency and transition to</a></p>&nbsp;&nbsp;           |
| <p><a href="https://towardsdatascience.com/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/2400/gradv/29/81/30/darken/25/1*AO2RBrRUBSqUpbysfzSEWA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Make it a habit</a></p>&nbsp;&nbsp;                                                                          |
| <p><a href="https://towardsdatascience.com/custom-training-loops-for-medical-image-segmentation-in-tensorflow-2-x-eb82a5426a67"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*oJ7plFZZvEeenqDi"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Dec 13, 2018</a></p>&nbsp;&nbsp;                                                                                            |
| <p><a href="https://towardsdatascience.com/pulling-an-all-nighter-e27b18ad96c7"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*gc_iYRaetVp4HFft"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Prerak Mody</a></p>&nbsp;&nbsp;                                                                                                                                            |
| <p><a href="https://towardsdatascience.com/real-time-age-gender-and-emotion-prediction-from-webcam-with-keras-and-opencv-bde6220d60a"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*MjIj7EuuQ2P3gcTayIkqwA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">David Moore</a></p>&nbsp;&nbsp;                                                                            |
| <p><a href="https://towardsdatascience.com/the-serendipitous-effectiveness-of-weight-decay-in-deep-learning-8b6b4234a3f9"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*yg7lLOo6SW-L6nrfmQynOQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Sun Weiran</a></p>&nbsp;&nbsp;                                                                                        |
| <p><a href="https://towardsdatascience.com/classical-classifier-combination-techniques-voting-approaches-borda-counts-and-5b047faaffbc"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*-pZlVc2On-Mimsg0P1-ZvA.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Antoine Labatie</a></p>&nbsp;&nbsp;                                                                       |
| <p><a href="https://towardsdatascience.com/keeping-accurate-time-fe0bf43e95ea"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*InaHZcZE4nTTuiCQPW50MQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Mahmoud Albardan</a></p>&nbsp;&nbsp;                                                                                                                             |
| <p><a href=""><img width="100" height="80" align='left' src=""                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">David Moore</a></p>&nbsp;&nbsp;                                                                                                                                                                                                                                                                         |

| title                                                                                        | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| disentangling feature and lazy training in deep neural networks	doi:10.1088/1742-5468/abc4de | two distinct limits for deep learning have been derived as the network width $h\rightarrow \infty$, depending on how the weights of the last layer scale with $h$. in the neural tangent kernel (ntk) limit, the dynamics becomes linear in the weights and is described by a frozen kernel $\theta$. by contrast, in the mean-field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. in this work we consider deep networks where the weights in the last layer scale as $\alpha h^{-1/2}$ at initialization. by varying $\alpha$ and $h$, we probe the crossover between the two limits. we observe the previously identified regimes of lazy training and feature training. in the lazy-training regime, the dynamics is almost linear and the ntk barely changes after initialization. the feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and learns some features. we perform numerical experiments on mnist, fashion-mnist, emnist and cifar10 and consider various architectures. we find that (i) the two regimes are separated by an $\alpha^*$ that scales as $h^{-1/2}$. (ii) network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) in both regimes, the fluctuations $\delta f$ induced on the learned function by initial conditions decay as $\delta f\sim 1/\sqrt{h}$, leading to a performance that increases with $h$. the same improvement can also be obtained at an intermediate width by ensemble-averaging several networks. (iv) in the feature-training regime we identify a time scale $t_1\sim\sqrt{h}\alpha$, such that for $t\ll t_1$ the dynamics is linear. |

| title                                                                                                                                | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|--------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| integrated information in the spiking-bursting stochastic model	doi:10.3390/e22121334                                                | this study presents a comprehensive analytic description in terms of the empirical "whole minus sum" version of integrated information in comparison to the "decoder based" version for the "spiking-bursting" discrete-time, discrete-state stochastic model, which was recently introduced to describe a specific type of dynamics in a neuron-astrocyte network. the "whole minus sum" information may change sign, and an interpretation of this transition in terms of "net synergy" is available in the literature. this motivates our particular interest to the sign of the "whole minus sum" information in our analytical consideration. the behavior of the "whole minus sum" and "decoder based" information measures are found to bear a lot of similarity, showing their mutual asymptotic convergence as time-uncorrelated activity is increased, with the sign transition of the "whole minus sum" information associated to a rapid growth in the "decoder based" information. the study aims at creating a theoretical base for using the spiking-bursting model as a well understood reference point for applying integrated information concepts to systems exhibiting similar bursting behavior (in particular, to neuron-astrocyte networks). the model can also be of interest as a new discrete-state test bench for different formulations of integrated information.                                                                                                                                                                                                      |
| review on biophysical modelling and simulation studies for transcranial   magnetic stimulation	doi:10.1088/1361-6560/aba40d          | transcranial magnetic stimulation (tms) is a technique for noninvasively stimulating a brain area for therapeutic, rehabilitation treatments and neuroscience research. despite our understanding of the physical principles and experimental developments pertaining to tms, it is difficult to identify the exact brain target as the generated dosage exhibits a non-uniform distribution owing to the complicated and subject-dependent brain anatomy and the lack of biomarkers that can quantify the effects of tms in most cortical areas. computational dosimetry has progressed significantly and enables tms assessment by computation of the induced electric field (the primary physical agent known to activate the brain neurons) in a digital representation of the human head. in this review, tms dosimetry studies are summarised, clarifying the importance of the anatomical and human biophysical parameters and computational methods. this review shows that there is a high consensus on the importance of a detailed cortical folding representation and an accurate modelling of the surrounding cerebrospinal fluid. recent studies have also enabled the prediction of individually optimised stimulation based on magnetic resonance imaging of the patient/subject and have attempted to understand the temporal effects of tms at the cellular level by incorporating neural modelling. these efforts, together with the fast deployment of personalised tms computations, will permit the adoption of tms dosimetry as a standard procedure in clinical procedures. |
| time cells might be optimized for predictive capacity, not redundancy   reduction or memory capacity	doi:10.1103/physreve.102.062404 | recently, researchers have found time cells in the hippocampus that appear to contain information about the timing of past events. some researchers have argued that time cells are taking a laplace transform of their input in order to reconstruct the past stimulus. we argue that stimulus prediction, not stimulus reconstruction or redundancy reduction, is in better agreement with observed responses of time cells. in the process, we introduce new analyses of nonlinear, continuous-time reservoirs that model these time cells.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| thermodynamic formalism in neuronal dynamics and spike train statistics	doi:10.3390/e22111330                                        | the thermodynamic formalism provides a rigorous mathematical framework to study quantitative and qualitative aspects of dynamical systems. at its core there is a variational principle corresponding, in its simplest form, to the maximum entropy principle. it is used as a statistical inference procedure to represent, by specific probability measures (gibbs measures), the collective behaviour of complex systems. this framework has found applications in different domains of science. in particular, it has been fruitful and influential in neurosciences. in this article, we review how the thermodynamic formalism can be exploited in the field of theoretical neuroscience, as a conceptual and operational tool, to link the dynamics of interacting neurons and the statistics of action potentials from either experimental data or mathematical models. we comment on perspectives and open problems in theoretical neuroscience that could be addressed within this formalism.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |