<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/the-effect-of-fast-shipping-80c2ccebbd13"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/1*UgHfoJY69YPO0EwBvc1tHQ.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">The Effect of Fast-shipping</a></p>&nbsp;&nbsp;                                                                                                                             |
| <p><a href="https://towardsdatascience.com/running-kedro-machine-learning-pipelines-with-google-cloud-bigquery-ml-47cfe2e7c943"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/0*F7oSsAnAqDkUIztD"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Running Kedro Machine Learning Pipelines with Google Cloud BigQuery ML</a></p>&nbsp;&nbsp;                                                  |
| <p><a href="https://towardsdatascience.com/advice-to-go-from-junior-to-senior-data-analyst-4680b7c126ed"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*DcfRmJwBIj7ntiGAQP932w.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Advice to Go From Junior to Senior Data Analyst</a></p>&nbsp;&nbsp;                                                                                     |
| <p><a href="https://towardsdatascience.com/training-autonomous-vehicles-using-augmented-random-search-in-carla-19fcbe62b697"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*_qFEYspVHIi1lDz6"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Training Autonomous Vehicles using Augmented Random Search in Carla</a></p>&nbsp;&nbsp;                                                         |
| <p><a href="https://towardsdatascience.com/lazy-predict-fit-and-evaluate-all-the-models-from-scikit-learn-with-a-single-line-of-code-7fe510c7281"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*iE_TSY3hhZji4TGkA67Rrw.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Lazy Predict: fit and evaluate all the models from scikit-learn with a single line of code</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/enabling-accurate-computer-vision-on-tiny-microcontrollers-with-rnnpool-operator-and-seedot-d6944930dcf9"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*3See_xIuXNv6VrzzN3-SNA.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Enabling Accurate Computer Vision on Tiny Microcontrollers with RNNPool Operator and</a></p>&nbsp;&nbsp;      |
| <p><a href="https://towardsdatascience.com/the-smart-hadoop-administrator-98528febcb65"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*ioEMnCJcbaGdzf0JaMX4AA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">The Smart Hadoop Administrator</a></p>&nbsp;&nbsp;                                                                                                                       |
| <p><a href="https://towardsdatascience.com/roc-curve-and-auc-from-scratch-in-numpy-visualized-2612bb9459ab"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*tysx4t6V6Rb5pptI"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">ROC Curve and AUC From Scratch in NumPy (Visualized!)</a></p>&nbsp;&nbsp;                                                                                        |
| <p><a href="https://towardsdatascience.com/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/2400/gradv/29/81/30/darken/25/1*AO2RBrRUBSqUpbysfzSEWA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Make it a habit</a></p>&nbsp;&nbsp;                                                                                           |
| <p><a href="https://towardsdatascience.com/sentiment-analysis-using-lstm-and-glove-embeddings-99223a87fe8e"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*ZTV0nki9dn47P6L93UKIcQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Dec 13, 2018</a></p>&nbsp;&nbsp;                                                                                                                      |
| <p><a href="https://towardsdatascience.com/understanding-residual-networks-resnets-intuitively-5afef9f089ea"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*Vf2Cau2cFd2z0x_QgDAlCQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Ketan Vaidya</a></p>&nbsp;&nbsp;                                                                                                                    |
| <p><a href="https://towardsdatascience.com/implementing-ssd-in-keras-part-i-network-structure-da3323f11cff"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*La_I2VXlENAJ9r0Wpf_vMg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Abhishek Verma</a></p>&nbsp;&nbsp;                                                                                                                    |
| <p><a href="https://towardsdatascience.com/emotion-recognition-4fba48dabb6e"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*DJwImDciI9gYWvJgs_L5MA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Socret Lee</a></p>&nbsp;&nbsp;                                                                                                                                                      |
| <p><a href="https://towardsdatascience.com/retrieving-similar-e-commerce-images-using-deep-learning-6d43ed05f46b"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*tAityLAyFUCNr1nZVQ0opQ.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Ketan Vaidya</a></p>&nbsp;&nbsp;                                                                                                                 |
| <p><a href="https://towardsdatascience.com/how-to-train-scaled-yolov4-to-detect-custom-objects-13f9077ebc89"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*i-n2dbPsve5qP_7f0YrngQ.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Abhishek Khatri</a></p>&nbsp;&nbsp;                                                                                                                  |
| <p><a href=""><img width="100" height="80" align='left' src=""                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Jacob Solawetz</a></p>&nbsp;&nbsp;                                                                                                                                                                                                                                                                                       |

| title                                                                                                            | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| frequency principle: fourier analysis sheds light on deep neural   networks	doi:10.4208/cicp.oa-2020-0085        | we study the training process of deep neural networks (dnns) from the fourier analysis perspective. we demonstrate a very universal frequency principle (f-principle) --- dnns often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets such as mnist/cifar10 and deep neural networks such as vgg16. this f-principle of dnns is opposite to the behavior of most conventional iterative numerical schemes (e.g., jacobi method), which exhibit faster convergence for higher frequencies for various scientific computing problems. with a simple theory, we illustrate that this f-principle results from the regularity of the commonly used activation functions. the f-principle implies an implicit bias that dnns tend to fit training data by a low-frequency function. this understanding provides an explanation of good generalization of dnns on most real datasets and bad generalization of dnns on parity function or randomized dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| disentangling feature and lazy training in deep neural networks	doi:10.1088/1742-5468/abc4de                     | two distinct limits for deep learning have been derived as the network width $h\rightarrow \infty$, depending on how the weights of the last layer scale with $h$. in the neural tangent kernel (ntk) limit, the dynamics becomes linear in the weights and is described by a frozen kernel $\theta$. by contrast, in the mean-field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. in this work we consider deep networks where the weights in the last layer scale as $\alpha h^{-1/2}$ at initialization. by varying $\alpha$ and $h$, we probe the crossover between the two limits. we observe the previously identified regimes of lazy training and feature training. in the lazy-training regime, the dynamics is almost linear and the ntk barely changes after initialization. the feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and learns some features. we perform numerical experiments on mnist, fashion-mnist, emnist and cifar10 and consider various architectures. we find that (i) the two regimes are separated by an $\alpha^*$ that scales as $h^{-1/2}$. (ii) network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) in both regimes, the fluctuations $\delta f$ induced on the learned function by initial conditions decay as $\delta f\sim 1/\sqrt{h}$, leading to a performance that increases with $h$. the same improvement can also be obtained at an intermediate width by ensemble-averaging several networks. (iv) in the feature-training regime we identify a time scale $t_1\sim\sqrt{h}\alpha$, such that for $t\ll t_1$ the dynamics is linear. |
| extracting dispersion curves from ambient noise correlations using deep   learning	doi:10.1109/tgrs.2020.2992043 | we present a machine-learning approach to classifying the phases of surface wave dispersion curves. standard ftan analysis of surfaces observed on an array of receivers is converted to an image, of which, each pixel is classified as fundamental mode, first overtone, or noise. we use a convolutional neural network (u-net) architecture with a supervised learning objective and incorporate transfer learning. the training is initially performed with synthetic data to learn coarse structure, followed by fine-tuning of the network using approximately 10% of the real data based on human classification. the results show that the machine classification is nearly identical to the human picked phases. expanding the method to process multiple images at once did not improve the performance. the developed technique will faciliate automated processing of large dispersion curve datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| dctrgan: improving the precision of generative models with reweighting	doi:10.1088/1748-0221/15/11/p11004        | significant advances in deep learning have led to more widely used and precise neural network-based generative models such as generative adversarial networks (gans). we introduce a post-hoc correction to deep generative models to further improve their fidelity, based on the deep neural networks using the classification for tuning and reweighting (dctr) protocol. the correction takes the form of a reweighting function that can be applied to generated examples when making predictions from the simulation. we illustrate this approach using gans trained on standard multimodal probability densities as well as calorimeter simulations from high energy physics. we show that the weighted gan examples significantly improve the accuracy of the generated samples without a large loss in statistical power. this approach could be applied to any generative model and is a promising refinement method for high energy physics applications and beyond.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| machine learning and computational mathematics	doi:10.4208/cicp.oa-2020-0185                                     | neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. this has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. at the same time, machine learning has also acquired the reputation of being a set of "black box" type of tricks, without fundamental principles. this has been a real obstacle for making further progress in machine learning. in this article, we try to address the following two very important questions: (1) how machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) how computational mathematics, particularly numerical analysis, {can} impact machine learning? we describe some of the most important progress that has been made on these issues. our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |

| title                                                                                                                                | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|--------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| occipital and left temporal eeg correlates of phenomenal consciousness	doi:10.1016/b978-0-12-802508-6.00018-1                        | in the first section, introduction, we present our experimental design. in the second section, we characterize the grand average occipital and temporal electrical activity correlated with a contrast in access. in the third section, we characterize the grand average occipital and temporal electrical activity correlated with a contrast in phenomenology and conclude characterizing the grand average occipital and temporal electrical activity co-occurring with unconsciousness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| integrated information in the spiking-bursting stochastic model	doi:10.3390/e22121334                                                | this study presents a comprehensive analytic description in terms of the empirical "whole minus sum" version of integrated information in comparison to the "decoder based" version for the "spiking-bursting" discrete-time, discrete-state stochastic model, which was recently introduced to describe a specific type of dynamics in a neuron-astrocyte network. the "whole minus sum" information may change sign, and an interpretation of this transition in terms of "net synergy" is available in the literature. this motivates our particular interest to the sign of the "whole minus sum" information in our analytical consideration. the behavior of the "whole minus sum" and "decoder based" information measures are found to bear a lot of similarity, showing their mutual asymptotic convergence as time-uncorrelated activity is increased, with the sign transition of the "whole minus sum" information associated to a rapid growth in the "decoder based" information. the study aims at creating a theoretical base for using the spiking-bursting model as a well understood reference point for applying integrated information concepts to systems exhibiting similar bursting behavior (in particular, to neuron-astrocyte networks). the model can also be of interest as a new discrete-state test bench for different formulations of integrated information.                                                                                                                                                                                                      |
| is there sufficient evidence for criticality in cortical systems?                                                                    | many studies have found evidence that the brain operates at a critical point, a processus known as self-organized criticality. a recent paper found remarkable scalings suggestive of criticality in systems as different as neural cultures, anesthetized or awake brains. we point out here that the diversity of these states would question any claimed role of criticality in information processing. furthermore, we show that two non-critical systems pass all the tests for criticality, a control that was not provided in the original article. we conclude that such false positives demonstrate that the presence of criticality in the brain is still not proven and that we need better methods that scaling analyses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| review on biophysical modelling and simulation studies for transcranial   magnetic stimulation	doi:10.1088/1361-6560/aba40d          | transcranial magnetic stimulation (tms) is a technique for noninvasively stimulating a brain area for therapeutic, rehabilitation treatments and neuroscience research. despite our understanding of the physical principles and experimental developments pertaining to tms, it is difficult to identify the exact brain target as the generated dosage exhibits a non-uniform distribution owing to the complicated and subject-dependent brain anatomy and the lack of biomarkers that can quantify the effects of tms in most cortical areas. computational dosimetry has progressed significantly and enables tms assessment by computation of the induced electric field (the primary physical agent known to activate the brain neurons) in a digital representation of the human head. in this review, tms dosimetry studies are summarised, clarifying the importance of the anatomical and human biophysical parameters and computational methods. this review shows that there is a high consensus on the importance of a detailed cortical folding representation and an accurate modelling of the surrounding cerebrospinal fluid. recent studies have also enabled the prediction of individually optimised stimulation based on magnetic resonance imaging of the patient/subject and have attempted to understand the temporal effects of tms at the cellular level by incorporating neural modelling. these efforts, together with the fast deployment of personalised tms computations, will permit the adoption of tms dosimetry as a standard procedure in clinical procedures. |
| efficiency of local learning rules in threshold-linear associative   networks                                                        | we derive the gardner storage capacity for associative networks of threshold linear units, and show that with hebbian learning they can operate closer to such gardner bound than binary networks, and even surpass it. this is largely achieved through a sparsification of the retrieved patterns, which we analyze for theoretical and empirical distributions of activity. as reaching the optimal capacity via non-local learning rules like backpropagation requires slow and neurally implausible training procedures, our results indicate that one-shot self-organized hebbian learning can be just as efficient.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| time cells might be optimized for predictive capacity, not redundancy   reduction or memory capacity	doi:10.1103/physreve.102.062404 | recently, researchers have found time cells in the hippocampus that appear to contain information about the timing of past events. some researchers have argued that time cells are taking a laplace transform of their input in order to reconstruct the past stimulus. we argue that stimulus prediction, not stimulus reconstruction or redundancy reduction, is in better agreement with observed responses of time cells. in the process, we introduce new analyses of nonlinear, continuous-time reservoirs that model these time cells.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| thermodynamic formalism in neuronal dynamics and spike train statistics	doi:10.3390/e22111330                                        | the thermodynamic formalism provides a rigorous mathematical framework to study quantitative and qualitative aspects of dynamical systems. at its core there is a variational principle corresponding, in its simplest form, to the maximum entropy principle. it is used as a statistical inference procedure to represent, by specific probability measures (gibbs measures), the collective behaviour of complex systems. this framework has found applications in different domains of science. in particular, it has been fruitful and influential in neurosciences. in this article, we review how the thermodynamic formalism can be exploited in the field of theoretical neuroscience, as a conceptual and operational tool, to link the dynamics of interacting neurons and the statistics of action potentials from either experimental data or mathematical models. we comment on perspectives and open problems in theoretical neuroscience that could be addressed within this formalism.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| real-time optimization of the current steering for visual prosthesis                                                                 | current steering on a multi-electrode array is commonly used to shape the electric field in the neural tissue in order to improve selectivity and efficacy of stimulation. previously, simulations of the electric field in tissue required separate computation for each set of the stimulation parameters. not only is this approach to modeling time-consuming and very difficult with a large number of electrodes, it is incompatible with real-time optimization of the current steering for practical applications. we present a framework for efficient computation of the electric field in the neural tissue based on superposition of the fields from a pre-calculated basis. such linear algebraic framework enables optimization of the current steering for any targeted electric field in real time. for applications to retinal prosthetics, we demonstrate how the stimulation depth can be optimized for each patient based on the retinal thickness and separation from the array, while maximizing the lateral confinement of the electric field essential for spatial resolution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| towards sample-efficient episodic control with dac-ml                                                                                | the sample-inefficiency problem in artificial intelligence refers to the inability of current deep reinforcement learning models to optimize action policies within a small number of episodes. recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in episodic reinforcement learning. however, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. in this paper, we capitalize on the design principles of the distributed adaptive control (dac) theory of mind and brain to build a novel cognitive architecture (dac-ml) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |