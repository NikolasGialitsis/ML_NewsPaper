<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                         |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/deploying-a-simple-ui-for-python-88e8e7cbbf61"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/0*qq1KYo_jJmLn2hjl"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Deploying a Simple UI</a></p>&nbsp;&nbsp;                                                        |
| <p><a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Hacking Super Intelligence</a></p>&nbsp;&nbsp;                                             |
| <p><a href="https://towardsdatascience.com/low-effort-machine-learning-tools-9622d7d57135"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*l-_uMYUimj73v-1JlAsXKg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Low Effort Machine</a></p>&nbsp;&nbsp;                                                |
| <p><a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Emerging problems in machine learning: making</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/ocr-and-the-wordsearch-solver-ai-515aeb816bdf"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*5UR8Yol9sE5x0j-T2oA_UQ.gif"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">OCR and the WordSearch</a></p>&nbsp;&nbsp;                                              |
| <p><a href="https://towardsdatascience.com/this-item-does-not-exist-2defbac76b39"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*0fFN0Q9xRrTTiz8X72olsw.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">This [item] Does</a></p>&nbsp;&nbsp;                                                          |
| <p><a href="https://towardsdatascience.com/three-functions-to-know-in-python-4f2d27a4d05"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*4D57GdFhAVyZWc4w"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Three Functions to Know</a></p>&nbsp;&nbsp;                                                       |
| <p><a href="https://towardsdatascience.com/machine-learning-in-production-95e1999bba84"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*AjJuGPwUTiB7zhyT"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Machine Learning in Production</a></p>&nbsp;&nbsp;                                                 |
| <p><a href="https://towardsdatascience.com/classifying-sentiment-from-text-reviews-a2c65ea468d6"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*PwmWEGSnv4fDlpsQc6qahw.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Classifying Sentiment from</a></p>&nbsp;&nbsp;                                   |
| <p><a href="https://towardsdatascience.com/altair-statistical-visualization-library-for-python-cfb63847c0c0"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*pv2AJGKWmqbFC9NjAWXuCg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Altair: Statistical Visualization Library</a></p>&nbsp;&nbsp;      |
| <p><a href="https://towardsdatascience.com/12-steps-for-beginner-to-pro-in-data-science-in-12-months-c6f6ba01f96e"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*hoGlPGRlScz1z1W4"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">12 Steps For Beginner To Pro In Data Science In</a></p>&nbsp;&nbsp;      |

| title                                                                                                          | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|----------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| convex geometry and duality of over-parameterized neural networks                                              | we develop a convex analytic approach to analyze finite width two-layer relu networks. we first prove that an optimal solution to the regularized training problem can be characterized as extreme points of a convex set, where simple solutions are encouraged via its convex geometrical properties. we then leverage this characterization to show that an optimal set of parameters yield linear spline interpolation for regression problems involving one dimensional or rank-one data. we also characterize the classification decision regions in terms of a kernel matrix and minimum $\ell_1$-norm solutions. this is in contrast to neural tangent kernel which is unable to explain predictions of finite width networks. our convex geometric characterization also provides intuitive explanations of hidden neurons as auto-encoders. in higher dimensions, we show that the training problem can be cast as a finite dimensional convex problem with infinitely many constraints. then, we apply certain convex relaxations and introduce a cutting-plane algorithm to globally optimize the network. we further analyze the exactness of the relaxations to provide conditions for the convergence to a global optimum. our analysis also shows that optimal network parameters can be also characterized as interpretable closed-form formulas in some practically relevant special cases.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| a review of machine learning applications in wildfire science and   management	doi:10.1139/er-2020-0019        | artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. since then the field has rapidly progressed congruently with the wide adoption of machine learning (ml) in the environmental sciences. here, we present a scoping review of ml in wildfire science and management. our objective is to improve awareness of ml among wildfire scientists and managers, as well as illustrate the challenging range of problems in wildfire science available to data scientists. we first present an overview of popular ml approaches used in wildfire science to date, and then review their use in wildfire science within six problem domains: 1) fuels characterization, fire detection, and mapping; 2) fire weather and climate change; 3) fire occurrence, susceptibility, and risk; 4) fire behavior prediction; 5) fire effects; and 6) fire management. we also discuss the advantages and limitations of various ml approaches and identify opportunities for future advances in wildfire science and management within a data science context. we identified 298 relevant publications, where the most frequently used ml methods included random forests, maxent, artificial neural networks, decision trees, support vector machines, and genetic algorithms. there exists opportunities to apply more current ml methods (e.g., deep learning and agent based learning) in wildfire science. however, despite the ability of ml models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ml methods requires sophisticated knowledge for their application. finally, we stress that the wildfire research and management community plays an active role in providing relevant, high quality data for use by practitioners of ml methods. |
| what were they thinking? pharmacologic priors implicit in a choice of   3+3 dose-escalation design             | if explicit, formal consideration of clinical pharmacology at all informs the design and conduct of modern oncology dose-finding trials, the designs themselves hardly attest to this. yet in conducting a trial, investigators affirm that they hold reasonable expectations of participant safety - expectations that necessarily depend on beliefs about how certain pharmacologic parameters are distributed in the study population. thus, these beliefs are implicit in a trial's presumed conformance to a community standard of safety, and may therefore to some extent be reverse-engineered from trial designs. for one popular form of dose-escalation trial design, i demonstrate here how this may be done.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| tight bounds on the smallest eigenvalue of the neural tangent kernel for   deep relu networks                  | a recent line of work has analyzed the theoretical properties of deep neural networks via the neural tangent kernel (ntk). in particular, the smallest eigenvalue of the ntk has been related to memorization capacity, convergence of gradient descent algorithms and generalization of deep nets. however, existing results either provide bounds in the two-layer setting or assume that the spectrum of the ntk is bounded away from 0 for multi-layer networks. in this paper, we provide tight bounds on the smallest eigenvalue of ntk matrices for deep relu networks, both in the limiting case of infinite widths and for finite widths. in the finite-width setting, the network architectures we consider are quite general: we require the existence of a wide layer with roughly order of $n$ neurons, $n$ being the number of data samples; and the scaling of the remaining widths is arbitrary (up to logarithmic factors). to obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of feature matrices, and upper bounds on the lipschitz constant of input-output feature maps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| nonparametric approximation of conditional expectation operators                                               | given a regular version of the joint distribution of two random variables $x,y$ on some second countable locally compact hausdorff space, we investigate the statistical approximation of the $l^2$-operator defined by $[pf](x) := \mathbb{e}[ f(y) \mid x = x ]$ under minimal assumptions. by modifying its domain, we prove that $p$ can be arbitrarily well approximated in operator norm by hilbert--schmidt operators acting on a reproducing kernel hilbert space. this fact allows to estimate $p$ uniformly by finite-rank operators over a dense subspace even when $p$ is not compact. in terms of modes of convergence, we thereby obtain the superiority of kernel-based techniques over classically used parametric projection approaches such as galerkin methods. this also provides a novel perspective on which limiting object the nonparametric estimate of $p$ converges to. as an application, we show that these results are particularly important for a large family of spectral analysis techniques for markov transition operators. our investigation also gives a new asymptotic perspective on the so-called kernel conditional mean embedding, which is the theoretical foundation of a wide variety of techniques in kernel-based nonparametric inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| on using classification datasets to evaluate graph outlier detection:   peculiar observations and new insights | it is common practice of the outlier mining community to repurpose classification datasets toward evaluating various detection models. to that end, often a binary classification dataset is used, where samples from (typically, the larger) one of the classes is designated as the inlier samples, and the other class is substantially down-sampled to create the (ground-truth) outlier samples. in this study, we identify an intriguing issue with repurposing graph classification datasets for graph outlier detection in this manner. surprisingly, the detection performance of outlier models depends significantly on which class is down-sampled; put differently, accuracy often flips from high to low depending on which of the classes is down-sampled to represent the outlier samples. the problem is notably exacerbated particularly for a certain family of propagation based outlier detection models. through careful analysis, we show that this issue mainly stems from disparate within-class sample similarity - which is amplified by various propagation based models - that impacts key characteristics of inlier/outlier distributions and indirectly, the difficulty of the outlier detection task and hence performance outcomes. with this study, we aim to draw attention to this (to our knowledge) previously-unnoticed issue, as it has implications for fair and effective evaluation of detection models, and hope that it will motivate the design of better evaluation benchmarks for outlier detection. finally, we discuss the possibly overarching implications of using propagation based models on datasets with disparate within-class sample similarity beyond outlier detection, specifically for graph classification and graph-level clustering tasks.                                                                                                                                                                     |
| a tight lower bound for uniformly stable algorithms                                                            | leveraging algorithmic stability to derive sharp generalization bounds is a classic and powerful approach in learning theory. since vapnik and chervonenkis [1974] first formalized the idea for analyzing svms, it has been utilized to study many fundamental learning algorithms (e.g., $k$-nearest neighbors [rogers and wagner, 1978], stochastic gradient method [hardt et al., 2016], linear regression [maurer, 2017], etc). in a recent line of great works by feldman and vondrak [2018, 2019] as well as bousquet et al. [2020b], they prove a high probability generalization upper bound of order $\widetilde{\mathcal{o}}(\gamma +\frac{l}{\sqrt{n}})$ for any uniformly $\gamma$-stable algorithm and $l$-bounded loss function. although much progress was achieved in proving generalization upper bounds for stable algorithms, our knowledge of lower bounds is rather limited. in fact, there is no nontrivial lower bound known ever since the study on uniform stability began [bousquet and elisseeff, 2002], to the best of our knowledge. in this paper we fill the gap by proving a tight generalization lower bound of order $\omega(\gamma+\frac{l}{\sqrt{n}})$, which matches the best known upper bound up to logarithmic factors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |

| title                                                                                                                                                                     | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| the role of consciousness in biological cybernetics: emergent adaptive   and maladaptive behaviours in artificial agents governed by the projective   consciousness model | the role of consciousness in biological cybernetics remains an essential yet open question. we applied the principles of the projective consciousness model (pcm) to derive a unified model of appraisal and social-affective perspective taking, and their role in the motivation of action. we show how the pcm can account for known relationships between appraisal and distance as an inverse distance law, and how it can be generalised to implement theory of mind. we used simulations of artificial agents based on psychological rationale to demonstrate how different model parameters could generate a variety of emergent adaptive and maladaptive behaviours: the ability to be resilient in the face of obstacles through imaginary projections, the emergence of social approach and joint attention behaviours, the ability to take advantage of false beliefs attributed to others, the emergence of avoidance behaviours as observed in social anxiety disorders, the presence of restricted interests as observed in autism spectrum disorders. the simulation of agents was applied to a specific robotic context, and agents' behaviours were demonstrated by controlling the corresponding robots. the approach opens new paths towards a science of consciousness, and applications, from clinical assessment and training to the design of artificial (virtual and robotic) agents. |
| nonequilibrium thermodynamics of input-driven networks                                                                                                                    | neural dynamics of energy-based models are governed by energy minimization and the patterns stored in the network are retrieved when the system reaches equilibrium. however, when the system is driven by time-varying external input, the nonequilibrium process of such physical system has not been well characterized. here, we study attractor neural networks, specifically the hopfield network, driven by time-varying external input and measure thermodynamic quantities along trajectories between two collective states. the overlap between distribution of the forward and reversal work along the nonequilibrium trajectories agrees with the equilibrium free energy difference between two states, following the prediction of crooks fluctuation theorem. we study conditions with different stimulation protocol and neural network constraints. we further discuss how biologically plausible synaptic connections and information processing may play a role in this nonequilibrium framework. these results demonstrate how nonequilibrium thermodynamics can be relevant for neural computation and connect to recent systems neuroscience studies with closed-loop dynamic perturbations.                                                                                                                                                                                             |
| using spatial logic and model checking for nevus segmentation                                                                                                             | spatial and spatio-temporal model checking techniques have a wide range of application domains, among which large scale distributed systems and signal and image analysis. in the latter domain, automatic and semi-automatic contouring in medical imaging has shown to be a very promising and versatile application that can greatly facilitate the work of professionals in this domain, while supporting explainability, easy replicability and exchange of medical image analysis methods. in recent work we have applied this model-checking technique to the (3d) contouring of tumours and related oedema in magnetic resonance images of the brain. in the current work we address the contouring of (2d) images of nevi. one of the challenges of treating nevi images is their considerable inhomogeneity in shape, colour, texture and size. to deal with this challenge we use a texture similarity operator, in combination with spatial logic operators. we apply our technique on images of a large public database and compare the results with associated ground truth segmentation provided by domain experts.                                                                                                                                                                                                                                                                             |