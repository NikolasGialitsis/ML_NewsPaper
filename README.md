<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Hacking Super Intelligence</a></p>&nbsp;&nbsp;                                                                                                              |
| <p><a href="https://towardsdatascience.com/low-effort-machine-learning-tools-9622d7d57135"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*l-_uMYUimj73v-1JlAsXKg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Low Effort Machine Learning Tools</a></p>&nbsp;&nbsp;                                                                                                |
| <p><a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Emerging problems in machine learning: making AI</a></p>&nbsp;&nbsp;                                                               |
| <p><a href="https://towardsdatascience.com/ocr-and-the-wordsearch-solver-ai-515aeb816bdf"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*5UR8Yol9sE5x0j-T2oA_UQ.gif"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">OCR and the WordSearch solver AI</a></p>&nbsp;&nbsp;                                                                                                   |
| <p><a href="https://towardsdatascience.com/what-is-model-complexity-compare-linear-regression-to-decision-trees-to-random-forests-7ec837b062a9"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*3cDEAWxZsO-erZoQ"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">What is Model Complexity? Compare Linear Regression to Decision Trees to Random Forests</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/finding-it-difficult-to-learn-programming-heres-why-639024be0a13"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*h4TLlisFk7XvtREAsS7C7Q.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Finding it difficult to learn programming? why.</a></p>&nbsp;&nbsp;                                                                |
| <p><a href="https://towardsdatascience.com/writing-a-custom-data-augmentation-layer-in-keras-2b53e048a98"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*QT928PwQat3jtFtH"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Writing a custom data augmentation layer in Keras</a></p>&nbsp;&nbsp;                                                                             |
| <p><a href="https://towardsdatascience.com/reinforcement-learning-sushi-go-238ad9bd7311"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*sSzzSj36bYJUWXbVPBTLbg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Reinforcement Learning & Sushi Go!</a></p>&nbsp;&nbsp;                                                                                                 |
| <p><a href="https://towardsdatascience.com/song-lyrics-generation-with-artificial-intelligence-rnn-cdba26738530"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*-OCwXTfs6Xwg_dAaR4xB1w.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Song lyrics generation with Artificial Intelligence (RNN)</a></p>&nbsp;&nbsp;                                                   |
| <p><a href="https://towardsdatascience.com/training-serving-skew-77d947c4c100"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*cLmdt2CQ0rzCRQ4sFJDtYA.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Training-serving skew</a></p>&nbsp;&nbsp;                                                                                                                         |

| title                                                                                                                       | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|-----------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| quantum-inspired canonical correlation analysis for exponentially large   dimensional data	doi:10.1016/j.neunet.2020.11.019 | canonical correlation analysis (cca) is a technique to find statistical dependencies between a pair of multivariate data. however, its application to high dimensional data is limited due to the resulting time complexity. while the conventional cca algorithm requires polynomial time, we have developed an algorithm that approximates cca with computational time proportional to the logarithm of the input dimensionality using quantum-inspired computation. the computational efficiency and approximation performance of the proposed quantum-inspired cca (qicca) algorithm are experimentally demonstrated. furthermore, the fast computation of qicca allows us to directly apply cca even after nonlinearly mapping raw input data into very high dimensional spaces. experiments performed using a benchmark dataset demonstrated that, by mapping the raw input data into the high dimensional spaces with second-order monomials, the proposed qicca extracted more correlations than linear cca and was comparable to deep cca and kernel cca. these results suggest that qicca is considerably useful and quantum-inspired computation has the potential to unlock a new field in which exponentially large dimensional data can be analyzed. |
| convolutions of totally positive distributions with applications to   kernel density estimation                             | in this work we study the estimation of the density of a totally positive random vector. total positivity of the distribution of a random vector implies a strong form of positive dependence between its coordinates and, in particular, it implies positive association. since estimating a totally positive density is a non-parametric problem, we take on a (modified) kernel density estimation approach. our main result is that the sum of scaled standard gaussian bumps centered at a min-max closed set provably yields a totally positive distribution. hence, our strategy for producing a totally positive estimator is to form the min-max closure of the set of samples, and output a sum of gaussian bumps centered at the points in this set. we can frame this sum as a convolution between the uniform distribution on a min-max closed set and a scaled standard gaussian. we further conjecture that convolving any totally positive density with a standard gaussian remains totally positive.                                                                                                                                                                                                                                              |
| deep sigma point processes                                                                                                  | we introduce deep sigma point processes, a class of parametric models inspired by the compositional structure of deep gaussian processes (dgps). deep sigma point processes (dspps) retain many of the attractive features of (variational) dgps, including mini-batch training and predictive uncertainty that is controlled by kernel basis functions. importantly, since dspps admit a simple maximum likelihood inference procedure, the resulting predictive distributions are not degraded by any posterior approximations. in an extensive empirical comparison on univariate and multivariate regression tasks we find that the resulting predictive distributions are significantly better calibrated than those obtained with other probabilistic methods for scalable regression, including variational dgps--often by as much as a nat per datapoint.                                                                                                                                                                                                                                                                                                                                                                                                  |
| kernel-based approximation of the koopman generator and schr\"odinger   operator                                            | many dimensionality and model reduction techniques rely on estimating dominant eigenfunctions of associated dynamical operators from data. important examples include the koopman operator and its generator, but also the schr\"odinger operator. we propose a kernel-based method for the approximation of differential operators in reproducing kernel hilbert spaces and show how eigenfunctions can be estimated by solving auxiliary matrix eigenvalue problems. the resulting algorithms are applied to molecular dynamics and quantum chemistry examples. furthermore, we exploit that, under certain conditions, the schr\"odinger operator can be transformed into a kolmogorov backward operator corresponding to a drift-diffusion process and vice versa. this allows us to apply methods developed for the analysis of high-dimensional stochastic differential equations to quantum mechanical systems.                                                                                                                                                                                                                                                                                                                                             |
| pac-bayes analysis beyond the usual bounds                                                                                  | we focus on a stochastic learning model where the learner observes a finite set of training examples and the output of the learning process is a data-dependent distribution over a space of hypotheses. the learned data-dependent distribution is then used to make randomized predictions, and the high-level theme addressed here is guaranteeing the quality of predictions on examples that were not seen during training, i.e. generalization. in this setting the unknown quantity of interest is the expected risk of the data-dependent randomized predictor, for which upper bounds can be derived via a pac-bayes analysis, leading to pac-bayes bounds.   specifically, we present a basic pac-bayes inequality for stochastic kernels, from which one may derive extensions of various known pac-bayes bounds as well as novel bounds. we clarify the role of the requirements of fixed 'data-free' priors, bounded losses, and i.i.d. data. we highlight that those requirements were used to upper-bound an exponential moment term, while the basic pac-bayes theorem remains valid without those restrictions. we present three bounds that illustrate the use of data-dependent priors, including one for the unbounded square loss.            |
| reproducible workflow                                                                                                       | reproducibility has been consistently identified as an important component of scientific research. although there is a general consensus on the importance of reproducibility along with the other commonly used 'r' terminology (i.e., replicability, repeatability etc.), there is some disagreement on the usage of these terms, including conflicting definitions used by different parts of the research community. in this encyclopedia article, we explore the different definitions used in scientific literature (specifically pertaining to computational research), whether there is a need for a single standardized definition and provide an alternative based on non-functional requirements. we also describe the role of reproducibility (and other r's) in scientific workflows.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| mathematical models of overparameterized neural networks	doi:10.1109/jproc.2020.3048020                                     | deep learning has received considerable empirical successes in recent years. however, while many ad hoc tricks have been discovered by practitioners, until recently, there has been a lack of theoretical understanding for tricks invented in the deep learning literature. known by practitioners that overparameterized neural networks are easy to learn, in the past few years there have been important theoretical developments in the analysis of overparameterized neural networks. in particular, it was shown that such systems behave like convex systems under various restricted settings, such as for two-layer nns, and when learning is restricted locally in the so-called neural tangent kernel space around specialized initializations. this paper discusses some of these recent progresses leading to significant better understanding of neural networks. we will focus on the analysis of two-layer neural networks, and explain the key mathematical models, with their algorithmic implications. we will then discuss challenges in understanding deep neural networks and some current research directions.                                                                                                                           |

| title                                                                                                         | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|---------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| occipital and left temporal eeg correlates of phenomenal consciousness	doi:10.1016/b978-0-12-802508-6.00018-1 | in the first section, introduction, we present our experimental design. in the second section, we characterize the grand average occipital and temporal electrical activity correlated with a contrast in access. in the third section, we characterize the grand average occipital and temporal electrical activity correlated with a contrast in phenomenology and conclude characterizing the grand average occipital and temporal electrical activity co-occurring with unconsciousness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| is there sufficient evidence for criticality in cortical systems?                                             | many studies have found evidence that the brain operates at a critical point, a processus known as self-organized criticality. a recent paper found remarkable scalings suggestive of criticality in systems as different as neural cultures, anesthetized or awake brains. we point out here that the diversity of these states would question any claimed role of criticality in information processing. furthermore, we show that two non-critical systems pass all the tests for criticality, a control that was not provided in the original article. we conclude that such false positives demonstrate that the presence of criticality in the brain is still not proven and that we need better methods that scaling analyses.                                                                                                                                                                                                                                                                                                                                                                  |
| efficiency of local learning rules in threshold-linear associative   networks                                 | we derive the gardner storage capacity for associative networks of threshold linear units, and show that with hebbian learning they can operate closer to such gardner bound than binary networks, and even surpass it. this is largely achieved through a sparsification of the retrieved patterns, which we analyze for theoretical and empirical distributions of activity. as reaching the optimal capacity via non-local learning rules like backpropagation requires slow and neurally implausible training procedures, our results indicate that one-shot self-organized hebbian learning can be just as efficient.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| real-time optimization of the current steering for visual prosthesis                                          | current steering on a multi-electrode array is commonly used to shape the electric field in the neural tissue in order to improve selectivity and efficacy of stimulation. previously, simulations of the electric field in tissue required separate computation for each set of the stimulation parameters. not only is this approach to modeling time-consuming and very difficult with a large number of electrodes, it is incompatible with real-time optimization of the current steering for practical applications. we present a framework for efficient computation of the electric field in the neural tissue based on superposition of the fields from a pre-calculated basis. such linear algebraic framework enables optimization of the current steering for any targeted electric field in real time. for applications to retinal prosthetics, we demonstrate how the stimulation depth can be optimized for each patient based on the retinal thickness and separation from the array, while maximizing the lateral confinement of the electric field essential for spatial resolution. |
| towards sample-efficient episodic control with dac-ml                                                         | the sample-inefficiency problem in artificial intelligence refers to the inability of current deep reinforcement learning models to optimize action policies within a small number of episodes. recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in episodic reinforcement learning. however, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. in this paper, we capitalize on the design principles of the distributed adaptive control (dac) theory of mind and brain to build a novel cognitive architecture (dac-ml) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task.                                                                                                                                                                                                            |