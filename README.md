| title                                                                                                                                                | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| tensor-based fusion of eeg and fmri to understand neurological changes   in schizophrenia	DOI:10.1109/iscas.2017.8050303                             | neuroimaging modalities such as functional magnetic resonance imaging (fmri) and electroencephalography (eeg) provide information about neurological functions in complementary spatiotemporal resolutions; therefore, fusion of these modalities is expected to provide better understanding of brain activity. in this paper, we jointly analyze fmri and multi-channel eeg signals collected during an auditory oddball task with the goal of capturing brain activity patterns that differ between patients with schizophrenia and healthy controls. rather than selecting a single electrode or matricizing the third-order tensor that can be naturally used to represent multi-channel eeg signals, we preserve the multi-way structure of eeg data and use a coupled matrix and tensor factorization (cmtf) model to jointly analyze fmri and eeg signals. our analysis reveals that (i) joint analysis of eeg and fmri using a cmtf model can capture meaningful temporal and spatial signatures of patterns that behave differently in patients and controls, and (ii) these differences and the interpretability of the associated components increase by including multiple electrodes from frontal, motor and parietal areas, but not necessarily by including all electrodes in the analysis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| accelerated parallel non-conjugate sampling for bayesian non-parametric   models                                                                     | inference of latent feature models in the bayesian nonparametric setting is generally difficult, especially in high dimensional settings, because it usually requires proposing features from some prior distribution. in special cases, where the integration is tractable, we can sample new feature assignments according to a predictive likelihood. however, this still may not be efficient in high dimensions. we present a novel method to accelerate the mixing of latent variable model inference by proposing feature locations based on the data, as opposed to the prior. first, we introduce an accelerated feature proposal mechanism that we show is a valid bayesian inference algorithm. next, we propose an approximate inference strategy to perform accelerated inference in parallel. a two-stage algorithm that combines the two approaches provides a computationally attractive method that exhibits good mixing behavior and converges to the posterior distribution of our model, while allowing us to exploit parallelization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| compressive sensing using iterative hard thresholding with low precision   data representation: theory and applications	DOI:10.1109/tsp.2020.3010355 | modern scientific instruments produce vast amounts of data, which can overwhelm the processing ability of computer systems. lossy compression of data is an intriguing solution, but comes with its own drawbacks, such as potential signal loss, and the need for careful optimization of the compression ratio. in this work, we focus on a setting where this problem is especially acute: compressive sensing frameworks for interferometry and medical imaging. we ask the following question: can the precision of the data representation be lowered for all inputs, with recovery guarantees and practical performance? our first contribution is a theoretical analysis of the normalized iterative hard thresholding (iht) algorithm when all input data, meaning both the measurement matrix and the observation vector are quantized aggressively. we present a variant of low precision normalized {iht} that, under mild conditions, can still provide recovery guarantees. the second contribution is the application of our quantization framework to radio astronomy and magnetic resonance imaging. we show that lowering the precision of the data can significantly accelerate image recovery. we evaluate our approach on telescope data and samples of brain images using cpu and fpga implementations achieving up to a 9x speed-up with negligible loss of recovery quality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| convex programming based spectral clustering                                                                                                         | clustering is a fundamental task in data analysis, and spectral clustering has been recognized as a promising approach to it. given a graph describing the relationship between data, spectral clustering explores the underlying cluster structure in two stages. the first stage embeds the nodes of the graph in real space, and the second stage groups the embedded nodes into several clusters. the use of the $k$-means method in the grouping stage is currently standard practice. we present a spectral clustering algorithm that uses convex programming in the grouping stage and study how well it works. this algorithm is designed based on the following observation. if a graph is well-clustered, then the nodes with the largest degree in each cluster can be found by computing an enclosing ellipsoid of the nodes embedded in real space, and the clusters can be identified by using those nodes. we show that, for well-clustered graphs, the algorithm can find clusters of nodes with minimal conductance. we also give an experimental assessment of the algorithm's performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| nonlocal flocking dynamics: learning the fractional order of pdes from   particle simulations	DOI:10.1007/s42967-019-00031-y                         | flocking refers to collective behavior of a large number of interacting entities, where the interactions between discrete individuals produce collective motion on the large scale. we employ an agent-based model to describe the microscopic dynamics of each individual in a flock, and use a fractional pde to model the evolution of macroscopic quantities of interest. the macroscopic models with phenomenological interaction functions are derived by applying the continuum hypothesis to the microscopic model. instead of specifying the fpdes with an ad hoc fractional order for nonlocal flocking dynamics, we learn the effective nonlocal influence function in fpdes directly from particle trajectories generated by the agent-based simulations. we demonstrate how the learning framework is used to connect the discrete agent-based model to the continuum fpdes in 1d and 2d nonlocal flocking dynamics. in particular, a cucker-smale particle model is employed to describe the microscale dynamics of each individual, while euler equations with nonlocal interaction terms are used to compute the evolution of macroscale quantities. the trajectories generated by the particle simulations mimic the field data of tracking logs that can be obtained experimentally. they can be used to learn the fractional order of the influence function using a gaussian process regression model implemented with the bayesian optimization. we show that the numerical solution of the learned euler equations solved by the finite volume scheme can yield correct density distributions consistent with the collective behavior of the agent-based system. the proposed method offers new insights on how to scale the discrete agent-based models to the continuum-based pde models, and could serve as a paradigm on extracting effective governing equations for nonlocal flocking dynamics directly from particle trajectories. |
| community detection in the sparse hypergraph stochastic block model                                                                                  | we consider the community detection problem in sparse random hypergraphs. angelini et al. (2015) conjectured the existence of a sharp threshold on model parameters for community detection in sparse hypergraphs generated by a hypergraph stochastic block model. we solve the positive part of the conjecture for the case of two blocks: above the threshold, there is a spectral algorithm which asymptotically almost surely constructs a partition of the hypergraph correlated with the true partition. our method is a generalization to random hypergraphs of the method developed by massouli\'{e} (2014) for sparse random graphs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| federated learning over wireless networks: convergence analysis and   resource allocation	DOI:10.1109/tnet.2020.3035770                              | there is an increasing interest in a fast-growing machine learning technique called federated learning, in which the model training is distributed over mobile user equipments (ues), exploiting ues' local computation and training data. despite its advantages in data privacy-preserving, federated learning (fl) still has challenges in heterogeneity across ues' data and physical resources. we first propose a fl algorithm which can handle the heterogeneous ues' data challenge without further assumptions except strongly convex and smooth loss functions. we provide the convergence rate characterizing the trade-off between local computation rounds of ue to update its local model and global communication rounds to update the fl global model. we then employ the proposed fl algorithm in wireless networks as a resource allocation optimization problem that captures the trade-off between the fl convergence wall clock time and energy consumption of ues with heterogeneous computing and power resources. even though the wireless resource allocation problem of fl is non-convex, we exploit this problem's structure to decompose it into three sub-problems and analyze their closed-form solutions as well as insights to problem design. finally, we illustrate the theoretical analysis for the new algorithm with tensorflow experiments and extensive numerical results for the wireless resource allocation sub-problems. the experiment results not only verify the theoretical convergence but also show that our proposed algorithm outperforms the vanilla fedavg algorithm in terms of convergence rate and testing accuracy.                                                                                                                                                                                                                                                                                    |
| a fourier analytical approach to estimation of smooth functions in   gaussian shift model                                                            | let $\mathbf{x}_j = \mathbf{\theta} + \mathbf{\epsilon}_j$, $j=1,\dots,n$ be i.i.d. copies of a gaussian random vector $\mathbf{x}\sim\mathcal{n}(\mathbf{\theta},\mathbf{\sigma})$ with unknown mean $\mathbf{\theta} \in \mathbb{r}^d$ and unknown covariance matrix $\mathbf{\sigma}\in \mathbb{r}^{d\times d}$. the goal of this article is to study the estimation of $f(\mathbf{\theta})$ where $f$ is a given smooth function of which smoothness is characterized by a besov-type norm. the problem of interest resides in the high dimensional regime where the intrinsic dimension can grow with the sample size $n$. inspired by the classical work of a. n. kolmogorov on unbiased estimation and littlewood-paley theory, we develop a new estimator based on a fourier analytical approach that achieves effective bias reduction. asymptotic normality and efficiency are proved when the smoothness index of $f$ is above certain threshold which was discovered recently by koltchinskii et. al. (2018) for a h\"{o}lder type class. numerical simulations are presented to validate our analysis. the simplicity of implementation and its superiority over the plug-in approach indicate the new estimator can be applied to a broad range of real world applications.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| flaml: a fast and lightweight automl library                                                                                                         | we study the problem of using low computational cost to automate the choices of learners and hyperparameters for an ad-hoc training dataset and error metric, by conducting trials of different configurations on the given training data. we investigate the joint impact of multiple factors on both trial cost and model error, and propose several design guidelines. following them, we build a fast and lightweight library flaml which optimizes for low computational resource in finding accurate models. flaml integrates several simple but effective search strategies into an adaptive system. it significantly outperforms top-ranked automl libraries on a large open source automl benchmark under equal, or sometimes orders of magnitude smaller budget constraints.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| radiounet: fast radio map estimation with convolutional neural networks                                                                              | in this paper we propose a highly efficient and very accurate deep learning method for estimating the propagation pathloss from a point $x$ (transmitter location) to any point $y$ on a planar domain. for applications such as user-cell site association and device-to-device link scheduling, an accurate knowledge of the pathloss function for all pairs of transmitter-receiver locations is very important. commonly used statistical models approximate the pathloss as a decaying function of the distance between transmitter and receiver. however, in realistic propagation environments characterized by the presence of buildings, street canyons, and objects at different heights, such radial-symmetric functions yield very misleading results. in this paper we show that properly designed and trained deep neural networks are able to learn how to estimate the pathloss function, given an urban environment, in a very accurate and computationally efficient manner. our proposed method, termed radiounet, learns from a physical simulation dataset, and generates pathloss estimations that are very close to the simulations, but are much faster to compute for real-time applications. moreover, we propose methods for transferring what was learned from simulations to real-life. numerical results show that our method significantly outperforms previously proposed methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| efficient crowdsourcing of crowd-generated microtasks	DOI:10.1371/journal.pone.0244245                                                               | allowing members of the crowd to propose novel microtasks for one another is an effective way to combine the efficiencies of traditional microtask work with the inventiveness and hypothesis generation potential of human workers. however, microtask proposal leads to a growing set of tasks that may overwhelm limited crowdsourcer resources. crowdsourcers can employ methods to utilize their resources efficiently, but algorithmic approaches to efficient crowdsourcing generally require a fixed task set of known size. in this paper, we introduce *cost forecasting* as a means for a crowdsourcer to use efficient crowdsourcing algorithms with a growing set of microtasks. cost forecasting allows the crowdsourcer to decide between eliciting new tasks from the crowd or receiving responses to existing tasks based on whether or not new tasks will cost less to complete than existing tasks, efficiently balancing resources as crowdsourcing occurs. experiments with real and synthetic crowdsourcing data show that cost forecasting leads to improved accuracy. accuracy and efficiency gains for crowd-generated microtasks hold the promise to further leverage the creativity and wisdom of the crowd, with applications such as generating more informative and diverse training data for machine learning applications and improving the performance of user-generated content and question-answering platforms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| consistent batch normalization for weighted loss in imbalanced-data   environment	DOI:10.1587/nolta.11.454                                           | in this study, classification problems based on feedforward neural networks in a data-imbalanced environment are considered. learning from an imbalanced dataset is one of the most important practical problems in the field of machine learning. a weighted loss function (wlf) based on a cost-sensitive approach is a well-known and effective method for imbalanced datasets. a combination of wlf and batch normalization (bn) is considered in this study. bn is considered as a powerful standard technique in the recent developments in deep learning. a simple combination of both methods leads to a size-inconsistency problem due to a mismatch between the interpretations of the effective size of the dataset in both methods. a simple modification to bn, called weighted bn (wbn), is proposed to correct the size mismatch. the idea of wbn is simple and natural. the proposed method in a data-imbalanced environment is validated using numerical experiments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| neighborhood structure assisted non-negative matrix factorization and   its application in unsupervised point-wise anomaly detection                 | dimensionality reduction is considered as an important step for ensuring competitive performance in unsupervised learning such as anomaly detection. non-negative matrix factorization (nmf) is a popular and widely used method to accomplish this goal. but nmf do not have the provision to include the neighborhood structure information and, as a result, may fail to provide satisfactory performance in presence of nonlinear manifold structure. to address that shortcoming, we propose to consider and incorporate the neighborhood structural similarity information within the nmf framework by modeling the data through a minimum spanning tree. we label the resulting method as the neighborhood structure assisted nmf. we further devise both offline and online algorithmic versions of the proposed method. empirical comparisons using twenty benchmark datasets as well as an industrial dataset extracted from a hydropower plant demonstrate the superiority of the neighborhood structure assisted nmf and support our claim of merit. looking closer into the formulation and properties of the neighborhood structure assisted nmf with other recent, enhanced versions of nmf reveals that inclusion of the neighborhood structure information using mst plays a key role in attaining the enhanced performance in anomaly detection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| gradient surgery for multi-task learning                                                                                                             | while deep learning and deep reinforcement learning (rl) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. however, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. the reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. in this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. we propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. on a series of challenging multi-task supervised and multi-task rl problems, this approach leads to substantial gains in efficiency and performance. further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| why do deep residual networks generalize better than deep feedforward   networks? -- a neural tangent kernel perspective                             | deep residual networks (resnets) have demonstrated better generalization performance than deep feedforward networks (ffnets). however, the theory behind such a phenomenon is still largely unknown. this paper studies this fundamental problem in deep learning from a so-called "neural tangent kernel" perspective. specifically, we first show that under proper conditions, as the width goes to infinity, training deep resnets can be viewed as learning reproducing kernel functions with some kernel function. we then compare the kernel of deep resnets with that of deep ffnets and discover that the class of functions induced by the kernel of ffnets is asymptotically not learnable, as the depth goes to infinity. in contrast, the class of functions induced by the kernel of resnets does not exhibit such degeneracy. our discovery partially justifies the advantages of deep resnets over deep ffnets in generalization abilities. numerical results are provided to support our claim.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| distributed non-convex optimization with sublinear speedup under   intermittent client availability                                                  | federated learning is a new distributed machine learning framework, where a bunch of heterogeneous clients collaboratively train a model without sharing training data. in this work, we consider a practical and ubiquitous issue when deploying federated learning in mobile environments: intermittent client availability, where the set of eligible clients may change during the training process. such intermittent client availability would seriously deteriorate the performance of the classical federated averaging algorithm (fedavg for short). thus, we propose a simple distributed non-convex optimization algorithm, called federated latest averaging (fedlaavg for short), which leverages the latest gradients of all clients, even when the clients are not available, to jointly update the global model in each iteration. our theoretical analysis shows that fedlaavg attains the convergence rate of $o(e^{1/2}/(n^{1/4} t^{1/2}))$, achieving a sublinear speedup with respect to the total number of clients. we implement fedlaavg along with several baselines and evaluate them over the benchmarking mnist and sentiment140 datasets. the evaluation results demonstrate that fedlaavg achieves more stable training than fedavg in both convex and non-convex settings and indeed reaches a sublinear speedup.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| coded federated learning	DOI:10.1109/gcwkshps45667.2019.9024521                                                                                      | federated learning is a method of training a global model from decentralized data distributed across client devices. here, model parameters are computed locally by each client device and exchanged with a central server, which aggregates the local models for a global view, without requiring sharing of training data. the convergence performance of federated learning is severely impacted in heterogeneous computing platforms such as those at the wireless edge, where straggling computations and communication links can significantly limit timely model parameter updates. this paper develops a novel coded computing technique for federated learning to mitigate the impact of stragglers. in the proposed coded federated learning (cfl) scheme, each client device privately generates parity training data and shares it with the central server only once at the start of the training phase. the central server can then preemptively perform redundant gradient computations on the composite parity data to compensate for the erased or delayed parameter updates. our results show that cfl allows the global model to converge nearly four times faster when compared to an uncoded approach                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| can implicit bias explain generalization? stochastic convex optimization   as a case study                                                           | the notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. this notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. we revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of stochastic gradient descent (sgd) in the context of stochastic convex optimization. as a first step, we provide a simple construction that rules out the existence of a \emph{distribution-independent} implicit regularizer that governs the generalization ability of sgd. we then demonstrate a learning problem that rules out a very general class of \emph{distribution-dependent} implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm's generalization performance by solely arguing about its implicit regularization properties.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| continual learning with node-importance based adaptive group sparse   regularization                                                                 | we propose a novel regularization-based continual learning method, dubbed as adaptive group sparsity based continual learning (ags-cl), using two group sparsity-based penalties. our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. by utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate efficient learning of new tasks. throughout the extensive experimental results, we show that our ags-cl uses much less additional memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| adaptive partial scanning transmission electron microscopy with   reinforcement learning                                                             | compressed sensing can decrease scanning transmission electron microscopy electron dose and scan time with minimal information loss. traditionally, sparse scans used in compressed sensing sample a static set of probing locations. however, dynamic scans that adapt to specimens are expected to be able to match or surpass the performance of static scans as static scans are a subset of possible dynamic scans. thus, we present a prototype for a contiguous sparse scan system that piecewise adapts scan paths to specimens as they are scanned. sampling directions for scan segments are chosen by a recurrent neural network based on previously observed scan segments. the recurrent neural network is trained by reinforcement learning to cooperate with a feedforward convolutional neural network that completes the sparse scans. this paper presents our learning policy, experiments, and example partial scans, and discusses future research directions. source code, pretrained models, and training data is openly accessible at https://github.com/jeffrey-ede/adaptive-scans                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| a survey on modern trainable activation functions                                                                                                    | in neural networks literature, there is a strong interest in identifying and defining activation functions which can improve neural network performance. in recent years there has been a renovated interest of the scientific community in investigating activation functions which can be trained during the learning process, usually referred to as "trainable", "learnable" or "adaptable" activation functions. they appear to lead to better network performance. diverse and heterogeneous models of trainable activation function have been proposed in the literature. in this paper, we present a survey of these models. starting from a discussion on the use of the term "activation function" in literature, we propose a taxonomy of trainable activation functions, highlight common and distinctive proprieties of recent and past models, and discuss main advantages and limitations of this type of approach. we show that many of the proposed approaches are equivalent to adding neuron layers which use fixed (non-trainable) activation functions and some simple local rule that constraints the corresponding weight layers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| bayesian nonparametric monotone regression	DOI:10.1002/env.2642                                                                                      | in many applications there is interest in estimating the relation between a predictor and an outcome when the relation is known to be monotone or otherwise constrained due to the physical processes involved. we consider one such application--inferring time-resolved aerosol concentration from a low-cost differential pressure sensor. the objective is to estimate a monotone function and make inference on the scaled first derivative of the function. we proposed bayesian nonparametric monotone regression which uses a bernstein polynomial basis to construct the regression function and puts a dirichlet process prior on the regression coefficients. the base measure of the dirichlet process is a finite mixture of a mass point at zero and a truncated normal. this construction imposes monotonicity while clustering the basis functions. clustering the basis functions reduces the parameter space and allows the estimated regression function to be linear. with the proposed approach we can make closed-formed inference on the derivative of the estimated function including full quantification of uncertainty. in a simulation study the proposed method performs similar to other monotone regression approaches when the true function is wavy but performs better when the true function is linear. we apply the method to estimate time-resolved aerosol concentration with a newly-developed portable aerosol monitor. the r package bnmr is made available to implement the method.                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| double generative adversarial networks for conditional independence   testing                                                                        | in this article, we consider the problem of high-dimensional conditional independence testing, which is a key building block in statistics and machine learning. we propose a double generative adversarial networks (gans)-based inference procedure. we first introduce a double gans framework to learn two generators, and integrate the two generators to construct a doubly-robust test statistic. we next consider multiple generalized covariance measures, and take their maximum as our test statistic. finally, we obtain the empirical distribution of our test statistic through multiplier bootstrap. we show that our test controls type-i error, while the power approaches one asymptotically. more importantly, these theoretical guarantees are obtained under much weaker and practically more feasible conditions compared to existing tests. we demonstrate the efficacy of our test through both synthetic and real datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| stealing deep reinforcement learning models for fun and profit                                                                                       | this paper presents the first model extraction attack against deep reinforcement learning (drl), which enables an external adversary to precisely recover a black-box drl model only from its interaction with the environment. model extraction attacks against supervised deep learning models have been widely studied. however, those techniques cannot be applied to the reinforcement learning scenario due to drl models' high complexity, stochasticity and limited observable information. we propose a novel methodology to overcome the above challenges. the key insight of our approach is that the process of drl model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box drl model only based on its predicted actions, and then leverages state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. experimental results indicate that our methodology can effectively recover the drl models with high fidelity and accuracy. we also demonstrate two use cases to show that our model extraction attack can (1) significantly improve the success rate of adversarial attacks, and (2) steal drl models stealthily even they are protected by dnn watermarks. these pose a severe threat to the intellectual property and privacy protection of drl applications.                                                                                                                                                                                                                                                                                                                                                                                                            |
| asymptotic errors for teacher-student convex generalized linear models   (or : how to prove kabashima's replica formula)                             | there has been a recent surge of interest in the study of asymptotic reconstruction performance in various cases of generalized linear estimation problems in the teacher-student setting, especially for the case of i.i.d standard normal matrices. here, we go beyond these matrices, and prove an analytical formula for the reconstruction performance of convex generalized linear models with rotationally-invariant data matrices with arbitrary bounded spectrum, rigorously confirming a conjecture originally derived using the replica method from statistical physics. the formula includes many problems such as compressed sensing or sparse logistic classification. the proof is achieved by leveraging on message passing algorithms and the statistical properties of their iterates, allowing to characterize the asymptotic empirical distribution of the estimator. our proof is crucially based on the construction of converging sequences of an oracle multi-layer vector approximate message passing algorithm, where the convergence analysis is done by checking the stability of an equivalent dynamical system. we illustrate our claim with numerical examples on mainstream learning methods such as sparse logistic regression and linear support vector classifiers, showing excellent agreement between moderate size simulation and the asymptotic prediction.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| categorical normalizing flows via continuous transformations                                                                                         | despite their popularity, to date, the application of normalizing flows on categorical data stays limited. the current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. in this paper, we investigate \emph{categorical normalizing flows}, that is normalizing flows for categorical data. by casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. as a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. based on categorical normalizing flows, we propose graphcnf a permutation-invariant generative model on graphs. graphcnf implements a three step approach modeling the nodes, edges and adjacency matrix stepwise to increase efficiency. on molecule generation, graphcnf outperforms both one-shot and autoregressive flow-based state-of-the-art.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| gpirt: a gaussian process model for item response theory                                                                                             | the goal of item response theoretic (irt) models is to provide estimates of latent traits from binary observed indicators and at the same time to learn the item response functions (irfs) that map from latent trait to observed response. however, in many cases observed behavior can deviate significantly from the parametric assumptions of traditional irt models. nonparametric irt models overcome these challenges by relaxing assumptions about the form of the irfs, but standard tools are unable to simultaneously estimate flexible irfs and recover ability estimates for respondents. we propose a bayesian nonparametric model that solves this problem by placing gaussian process priors on the latent functions defining the irfs. this allows us to simultaneously relax assumptions about the shape of the irfs while preserving the ability to estimate latent traits. this in turn allows us to easily extend the model to further tasks such as active learning. gpirt therefore provides a simple and intuitive solution to several longstanding problems in the irt literature.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| a novel framework for spatio-temporal prediction of environmental data   using deep learning	DOI:10.1038/s41598-020-79148-7                          | as the role played by statistical and computational sciences in climate and environmental modelling and prediction becomes more important, machine learning researchers are becoming more aware of the relevance of their work to help tackle the climate crisis. indeed, being universal nonlinear function approximation tools, machine learning algorithms are efficient in analysing and modelling spatially and temporally variable environmental data. while deep learning models have proved to be able to capture spatial, temporal, and spatio-temporal dependencies through their automatic feature representation learning, the problem of the interpolation of continuous spatio-temporal fields measured on a set of irregular points in space is still under-investigated. to fill this gap, we introduce here a framework for spatio-temporal prediction of climate and environmental data using deep learning. specifically, we show how spatio-temporal processes can be decomposed in terms of a sum of products of temporally referenced basis functions, and of stochastic spatial coefficients which can be spatially modelled and mapped on a regular grid, allowing the reconstruction of the complete spatio-temporal signal. applications on two case studies based on simulated and real-world data will show the effectiveness of the proposed framework in modelling coherent spatio-temporal fields.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| deep active learning for solvability prediction in power systems                                                                                     | traditional methods for solvability region analysis can only have inner approximations with inconclusive conservatism. machine learning methods have been proposed to approach the real region. in this letter, we propose a deep active learning framework for power system solvability prediction. compared with the passive learning methods where the training is performed after all instances are labeled, the active learning selects most informative instances to be label and therefore significantly reduce the size of labeled dataset for training. in the active learning framework, the acquisition functions, which correspond to different sampling strategies, are defined in terms of the on-the-fly posterior probability from the classifier. the ieee 39-bus system is employed to validate the proposed framework, where a two-dimensional case is illustrated to visualize the effectiveness of the sampling method followed by the full-dimensional numerical experiments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| hopfield networks is all you need                                                                                                                    | we introduce a modern hopfield network with continuous states and a corresponding update rule. the new hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. it has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. the new update rule is equivalent to the attention mechanism used in transformers. this equivalence enables a characterization of the heads of transformer models. these heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. the new modern hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. these hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. we demonstrate the broad applicability of the hopfield layers across various domains. hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. on the uci benchmark collections of small classification tasks, where deep learning methods typically struggle, hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. finally, hopfield layers achieved state-of-the-art on two drug design datasets. the implementation is available at: https://github.com/ml-jku/hopfield-layers                                      |
| quaternion graph neural networks                                                                                                                     | recently, graph neural networks (gnns) become a principal research direction to learn low-dimensional continuous embeddings of nodes and graphs to predict node and graph labels, respectively. however, euclidean embeddings have high distortion when using gnns to model complex graphs such as social networks. furthermore, existing gnns are not very efficient with the high number of model parameters when increasing the number of hidden layers. therefore, we move beyond the euclidean space to a hyper-complex vector space to improve graph representation quality and reduce the number of model parameters. to this end, we propose quaternion graph neural networks (qgnn) to generalize gcns within the quaternion space to learn quaternion embeddings for nodes and graphs. the quaternion space, a hyper-complex vector space, provides highly meaningful computations through hamilton product compared to the euclidean and complex vector spaces. as a result, our qgnn can reduce the model size up to four times and enhance learning better graph representations. experimental results show that the proposed qgnn produces state-of-the-art accuracies on a range of well-known benchmark datasets for three downstream tasks, including graph classification, semi-supervised node classification, and text (node) classification. our code is available at: https://github.com/daiquocnguyen/qgnn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| how to put users in control of their data in federated top-n   recommendation with learning to rank	DOI:10.1145/3412841.3442010                      | recommendation services are extensively adopted in several user-centered applications as a tool to alleviate the information overload problem and help users in orienteering in a vast space of possible choices. in such scenarios, data ownership is a crucial concern since users may not be willing to share their sensitive preferences (e.g., visited locations) with a central server. unfortunately, data harvesting and collection is at the basis of modern, state-of-the-art approaches to recommendation. to address this issue, we present fpl, an architecture in which users collaborate in training a central factorization model while controlling the amount of sensitive data leaving their devices. the proposed approach implements pair-wise learning-to-rank optimization by following the federated learning principles, originally conceived to mitigate the privacy risks of traditional machine learning. the public implementation is available at https://split.to/sisinflab-fpl.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| a new role for circuit expansion for learning in neural networks                                                                                     | many sensory pathways in the brain rely on sparsely active populations of neurons downstream from the input stimuli. the biological reason for the occurrence of expanded structure in the brain is unclear, but may be because expansion can increase the expressive power of a neural network. in this work, we show that expanding a neural network can improve its generalization performance even in cases in which the expanded structure is pruned after the learning period. to study this setting we use a teacher-student framework where a perceptron teacher network generates labels which are corrupted with small amounts of noise. we then train a student network that is structurally matched to the teacher and can achieve optimal accuracy if given the teacher's synaptic weights. we find that sparse expansion of the input of a student perceptron network both increases its capacity and improves the generalization performance of the network when learning a noisy rule from a teacher perceptron when these expansions are pruned after learning. we find similar behavior when the expanded units are stochastic and uncorrelated with the input and analyze this network in the mean field limit. we show by solving the mean field equations that the generalization error of the stochastic expanded student network continues to drop as the size of the network increases. the improvement in generalization performance occurs despite the increased complexity of the student network relative to the teacher it is trying to learn. we show that this effect is closely related to the addition of slack variables in artificial neural networks and suggest possible implications for artificial and biological neural networks.                                                                                                                                                                                       |
| stochastic markov gradient descent and training low-bit neural networks                                                                              | the massive size of modern neural networks has motivated substantial recent interest in neural network quantization. we introduce stochastic markov gradient descent (smgd), a discrete optimization method applicable to training quantized neural networks. the smgd algorithm is designed for settings where memory is highly constrained during training. we provide theoretical guarantees of algorithm performance as well as encouraging numerical results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| on communication compression for distributed optimization on   heterogeneous data                                                                    | lossy gradient compression, with either unbiased or biased compressors, has become a key tool to avoid the communication bottleneck in centrally coordinated distributed training of machine learning models. we analyze the performance of two standard and general types of methods: (i) distributed quantized sgd (d-qsgd) with arbitrary unbiased quantizers and (ii) distributed sgd with error-feedback and biased compressors (d-ef-sgd) in the heterogeneous (non-iid) data setting. our results indicate that d-ef-sgd is much less affected than d-qsgd by non-iid data, but both methods can suffer a slowdown if data-skewness is high. we further study two alternatives that are not (or much less) affected by heterogenous data distributions: first, a recently proposed method that is effective on strongly convex problems, and secondly, we point out a more general approach that is applicable to linear compressors only but effective in all considered scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| effective federated adaptive gradient methods with non-iid decentralized   data                                                                      | federated learning allows loads of edge computing devices to collaboratively learn a global model without data sharing. the analysis with partial device participation under non-iid and unbalanced data reflects more reality. in this work, we propose federated learning versions of adaptive gradient methods - federated agms - which employ both the first-order and second-order momenta, to alleviate generalization performance deterioration caused by dissimilarity of data population among devices. to further improve the test performance, we compare several schemes of calibration for the adaptive learning rate, including the standard adam calibrated by $\epsilon$, $p$-adam, and one calibrated by an activation function. our analysis provides the first set of theoretical results that the proposed (calibrated) federated agms converge to a first-order stationary point under non-iid and unbalanced data settings for nonconvex optimization. we perform extensive experiments to compare these federated learning methods with the state-of-the-art fedavg, fedmomentum and scaffold and to assess the different calibration schemes and the advantages of agms over the current federated learning methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| experimental design for overparameterized learning with application to   single shot deep active learning                                            | the impressive performance exhibited by modern machine learning models hinges on the ability to train such models on a very large amount of labeled data. however, since access to large volumes of labeled data is often limited or expensive, it is desirable to alleviate this bottleneck by carefully curating the training set. optimal experimental design is a well-established paradigm for selecting data point to be labeled so to maximally inform the learning process. unfortunately, classical theory on optimal experimental design focuses on selecting examples in order to learn underparameterized (and thus, non-interpolative) models, while modern machine learning models such as deep neural networks are overparameterized, and oftentimes are trained to be interpolative. as such, classical experimental design methods are not applicable in many modern learning setups. indeed, the predictive performance of underparameterized models tends to be variance dominated, so classical experimental design focuses on variance reduction, while the predictive performance of overparameterized models can also be, as is shown in this paper, bias dominated or of mixed nature. in this paper we propose a design strategy that is well suited for overparameterized regression and interpolation. we demonstrate the applicability of our method in the context of deep learning by proposing a new algorithm for single-shot deep active learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| mastering atari with discrete world models                                                                                                           | intelligent agents need to generalize from past experience to achieve goals in complex environments. world models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. while learning world models from image inputs has recently become feasible for some tasks, modeling atari games accurately enough to derive successful behaviors has remained an open challenge for many years. we introduce dreamerv2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. the world model uses discrete representations and is trained separately from the policy. dreamerv2 constitutes the first agent that achieves human-level performance on the atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. with the same computational budget and wall-clock time, dreamerv2 reaches 200m frames and exceeds the final performance of the top single-gpu agents iqn and rainbow.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ensure: ensemble stein's unbiased risk estimator for unsupervised   learning                                                                         | deep learning algorithms are emerging as powerful alternatives to compressed sensing methods, offering improved image quality and computational efficiency. unfortunately, fully sampled training images may not be available or are difficult to acquire in several applications, including high-resolution and dynamic imaging. previous studies in image reconstruction have utilized stein's unbiased risk estimator (sure) as a mean square error (mse) estimate for the image denoising step in an unrolled network. unfortunately, the end-to-end training of a network using sure remains challenging since the projected sure loss is a poor approximation to the mse, especially in the heavily undersampled setting. we propose an ensemble sure (ensure) approach to train a deep network only from undersampled measurements. in particular, we show that training a network using an ensemble of images, each acquired with a different sampling pattern, can closely approximate the mse. our preliminary experimental results show that the proposed ensure approach gives comparable reconstruction quality to supervised learning and a recent unsupervised learning method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| differentially private weighted sampling                                                                                                             | common datasets have the form of elements with keys (e.g., transactions and products) and the goal is to perform analytics on the aggregated form of key and frequency pairs. a weighted sample of keys by (a function of) frequency is a highly versatile summary that provides a sparse set of representative keys and supports approximate evaluations of query statistics. we propose private weighted sampling (pws): a method that ensures element-level differential privacy while retaining, to the extent possible, the utility of a respective non-private weighted sample. pws maximizes the reporting probabilities of keys and estimation quality of a broad family of statistics. pws improves over the state of the art also for the well-studied special case of private histograms, when no sampling is performed. we empirically demonstrate significant performance gains compared with prior baselines: 20%-300% increase in key reporting for common zipfian frequency distributions and accuracy for $\times 2$-$ 8$ lower frequencies in estimation tasks. moreover, pws is applied as a simple post-processing of a non-private sample, without requiring the original data. this allows for seamless integration with existing implementations of non-private schemes and retaining the efficiency of schemes designed for resource-constrained settings such as massive distributed or streamed data. we believe that due to practicality and performance, pws may become a method of choice in applications where privacy is desired.                                                                                                                                                                                                                                                                                                                                                                                                |
| automated hyperparameter selection for the pc algorithm                                                                                              | the pc algorithm infers causal relations using conditional independence tests that require a pre-specified type i $\alpha$ level. pc is however unsupervised, so we cannot tune $\alpha$ using traditional cross-validation. we therefore propose autopc, a fast procedure that optimizes $\alpha$ directly for a user chosen metric. we in particular force pc to double check its output by executing a second run on the recovered graph. we choose the final output as the one which maximizes stability between the two runs. autopc consistently outperforms the state of the art across multiple metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| concentration inequalities for statistical inference                                                                                                 | this paper gives a review of concentration inequalities which are widely employed in non-asymptotical analyses of mathematical statistics in a wide range of settings, from distribution-free to distribution-dependent, from sub-gaussian to sub-exponential, sub-gamma, and sub-weibull random variables, and from the mean to the maximum concentration. this review provides results in these settings with some fresh new results. given the increasing popularity of high-dimensional data and inference, results in the context of high-dimensional linear and poisson regressions are also provided. we aim to illustrate the concentration inequalities with known constants and to improve existing bounds with sharper constants.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| a case for new neural network smoothness constraints                                                                                                 | how sensitive should machine learning models be to input changes? we tackle the question of model smoothness and show that it is a useful inductive bias which aids generalization, adversarial robustness, generative modeling and reinforcement learning. we explore current methods of imposing smoothness constraints and observe they lack the flexibility to adapt to new tasks, they don't account for data modalities, they interact with losses, architectures and optimization in ways not yet fully understood. we conclude that new advances in the field are hinging on finding ways to incorporate data, tasks and learning into our definitions of smoothness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| aging bandits: regret analysis and order-optimal learning algorithm for   wireless networks with stochastic arrivals                                 | we consider a single-hop wireless network with sources transmitting time-sensitive information to the destination over multiple unreliable channels. packets from each source are generated according to a stochastic process with known statistics and the state of each wireless channel (on/off) varies according to a stochastic process with unknown statistics. the reliability of the wireless channels is to be learned through observation. at every time slot, the learning algorithm selects a single pair (source, channel) and the selected source attempts to transmit its packet via the selected channel. the probability of a successful transmission to the destination depends on the reliability of the selected channel. the goal of the learning algorithm is to minimize the age-of-information (aoi) in the network over $t$ time slots. to analyze the performance of the learning algorithm, we introduce the notion of aoi regret, which is the difference between the expected cumulative aoi of the learning algorithm under consideration and the expected cumulative aoi of a genie algorithm that knows the reliability of the channels a priori. the aoi regret captures the penalty incurred by having to learn the statistics of the channels over the $t$ time slots. the results are two-fold: first, we consider learning algorithms that employ well-known solutions to the stochastic multi-armed bandit problem (such as $\epsilon$-greedy, upper confidence bound, and thompson sampling) and show that their aoi regret scales as $\theta(\log t)$; second, we develop a novel learning algorithm and show that it has $o(1)$ regret. to the best of our knowledge, this is the first learning algorithm with bounded aoi regret.                                                                                                                                                                                    |
| fast physical activity suggestions: efficient hyperparameter learning in   mobile health                                                             | users can be supported to adopt healthy behaviors, such as regular physical activity, via relevant and timely suggestions on their mobile devices. recently, reinforcement learning algorithms have been found to be effective for learning the optimal context under which to provide suggestions. however, these algorithms are not necessarily designed for the constraints posed by mobile health (mhealth) settings, that they be efficient, domain-informed and computationally affordable. we propose an algorithm for providing physical activity suggestions in mhealth settings. using domain-science, we formulate a contextual bandit algorithm which makes use of a linear mixed effects model. we then introduce a procedure to efficiently perform hyper-parameter updating, using far less computational resources than competing approaches. not only is our approach computationally efficient, it is also easily implemented with closed form matrix algebraic updates and we show improvements over state of the art approaches both in speed and accuracy of up to 99% and 56% respectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| tight bounds on the smallest eigenvalue of the neural tangent kernel for   deep relu networks                                                        | a recent line of work has analyzed the theoretical properties of deep neural networks via the neural tangent kernel (ntk). in particular, the smallest eigenvalue of the ntk has been related to memorization capacity, convergence of gradient descent algorithms and generalization of deep nets. however, existing results either provide bounds in the two-layer setting or assume that the spectrum of the ntk is bounded away from 0 for multi-layer networks. in this paper, we provide tight bounds on the smallest eigenvalue of ntk matrices for deep relu networks, both in the limiting case of infinite widths and for finite widths. in the finite-width setting, the network architectures we consider are quite general: we require the existence of a wide layer with roughly order of $n$ neurons, $n$ being the number of data samples; and the scaling of the remaining widths is arbitrary (up to logarithmic factors). to obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of feature matrices, and upper bounds on the lipschitz constant of input-output feature maps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| empirical bayes pca in high dimensions                                                                                                               | when the dimension of data is comparable to or larger than the number of available data samples, principal components analysis (pca) is known to exhibit problematic phenomena of high-dimensional noise. in this work, we propose an empirical bayes pca method that reduces this noise by estimating a structural prior for the joint distributions of the principal components. this eb-pca method is based upon the classical kiefer-wolfowitz nonparametric mle for empirical bayes estimation, distributional results derived from random matrix theory for the sample pcs, and iterative refinement using an approximate message passing (amp) algorithm. in theoretical "spiked" models, eb-pca achieves bayes-optimal estimation accuracy in the same settings as the oracle bayes amp procedure that knows the true priors. empirically, eb-pca can substantially improve over pca when there is strong prior structure, both in simulation and on several quantitative benchmarks constructed using data from the 1000 genomes project and the international hapmap project. a final illustration is presented for an analysis of gene expression data obtained by single-cell rna-seq.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ordered counterfactual explanation by mixed-integer linear optimization                                                                              | post-hoc explanation methods for machine learning models have been widely used to support decision-making. one of the popular methods is counterfactual explanation (ce), which provides a user with a perturbation vector of features that alters the prediction result. given a perturbation vector, a user can interpret it as an "action" for obtaining one's desired decision result. in practice, however, showing only a perturbation vector is often insufficient for users to execute the action. the reason is that if there is an asymmetric interaction among features, such as causality, the total cost of the action is expected to depend on the order of changing features. therefore, practical ce methods are required to provide an appropriate order of changing features in addition to a perturbation vector. for this purpose, we propose a new framework called ordered counterfactual explanation (ordce). we introduce a new objective function that evaluates a pair of an action and an order based on feature interaction. to extract an optimal pair, we propose a mixed-integer linear optimization approach with our objective function. numerical experiments on real datasets demonstrated the effectiveness of our ordce in comparison with unordered ce methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| estimation in nonparametric regression model with additive and   multiplicative noise via laguerre series                                            | we look into the nonparametric regression estimation with additive and multiplicative noise and construct adaptive thresholding estimators based on laguerre series. the proposed approach achieves asymptotically near-optimal convergence rates when the unknown function belongs to laguerre-sobolev space. we consider the problem under two noise structures; (1) { i.i.d.} gaussian errors and (2) long-memory gaussian errors. in the { i.i.d.} case, our convergence rates are similar to those found in the literature. in the long-memory case, the convergence rates depend on the long-memory parameters only when long-memory is strong enough in either noise source, otherwise, the rates are identical to those under { i.i.d.} noise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| selective forgetting of deep networks at a finer level than samples                                                                                  | selective forgetting or removing information from deep neural networks (dnns) is essential for continuous learning and is challenging in controlling the dnns. such forgetting is crucial also in a practical sense since the deployed dnns may be trained on the data with outliers, poisoned by attackers, or with leaked/sensitive information. in this paper, we formulate selective forgetting for classification tasks at a finer level than the samples' level. we specify the finer level based on four datasets distinguished by two conditions: whether they contain information to be forgotten and whether they are available for the forgetting procedure. additionally, we reveal the need for such formulation with the datasets by showing concrete and practical situations. moreover, we introduce the forgetting procedure as an optimization problem on three criteria; the forgetting, the correction, and the remembering term. experimental results show that the proposed methods can make the model forget to use specific information for classification. notably, in specific cases, our methods improved the model's accuracy on the datasets, which contains information to be forgotten but is unavailable in the forgetting procedure. such data are unexpectedly found and misclassified in actual situations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| learning structures in earth observation data with gaussian processes	DOI:10.1007/978-3-319-44412-3_6                                                | gaussian processes (gps) has experienced tremendous success in geoscience in general and for bio-geophysical parameter retrieval in the last years. gps constitute a solid bayesian framework to formulate many function approximation problems consistently. this paper reviews the main theoretical gp developments in the field. we review new algorithms that respect the signal and noise characteristics, that provide feature rankings automatically, and that allow applicability of associated uncertainty intervals to transport gp models in space and time. all these developments are illustrated in the field of geoscience and remote sensing at a local and global scales through a set of illustrative examples.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| finding global minima via kernel approximations                                                                                                      | we consider the global minimization of smooth functions based solely on function evaluations. algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. in this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. this is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. leveraging recent representation properties of reproducing kernel hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum.   given $n$ samples, the computational cost is $o(n^{3.5})$ in time, $o(n^2)$ in space, and we achieve a convergence rate to the global optimum that is $o(n^{-m/d + 1/2 + 3/d})$ where $m$ is the degree of differentiability of the function and $d$ the number of dimensions. the rate is nearly optimal in the case of sobolev functions and more generally makes the proposed method particularly suitable for functions that have a large number of derivatives. indeed, when $m$ is in the order of $d$, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper).                                                                                                                                                                                                                                                                                                                                                        |
| unsupervised functional data analysis via nonlinear dimension reduction                                                                              | in recent years, manifold methods have moved into focus as tools for dimension reduction. assuming that the high-dimensional data actually lie on or close to a low-dimensional nonlinear manifold, these methods have shown convincing results in several settings. this manifold assumption is often reasonable for functional data, i.e., data representing continuously observed functions, as well. however, the performance of manifold methods recently proposed for tabular or image data has not been systematically assessed in the case of functional data yet. moreover, it is unclear how to evaluate the quality of learned embeddings that do not yield invertible mappings, since the reconstruction error cannot be used as a performance measure for such representations. in this work, we describe and investigate the specific challenges for nonlinear dimension reduction posed by the functional data setting. the contributions of the paper are three-fold: first of all, we define a theoretical framework which allows to systematically assess specific challenges that arise in the functional data context, transfer several nonlinear dimension reduction methods for tabular and image data to functional data, and show that manifold methods can be used successfully in this setting. secondly, we subject performance assessment and tuning strategies to a thorough and systematic evaluation based on several different functional data settings and point out some previously undescribed weaknesses and pitfalls which can jeopardize reliable judgment of embedding quality. thirdly, we propose a nuanced approach to make trustworthy decisions for or against competing nonconforming embeddings more objectively.                                                                                                                                                                                                 |
| data assimilation in the latent space of a neural network                                                                                            | there is an urgent need to build models to tackle indoor air quality issue. since the model should be accurate and fast, reduced order modelling technique is used to reduce the dimensionality of the problem. the accuracy of the model, that represent a dynamic system, is improved integrating real data coming from sensors using data assimilation techniques. in this paper, we formulate a new methodology called latent assimilation that combines data assimilation and machine learning. we use a convolutional neural network to reduce the dimensionality of the problem, a long-short-term-memory to build a surrogate model of the dynamic system and an optimal interpolated kalman filter to incorporate real data. experimental results are provided for co2 concentration within an indoor space. this methodology can be used for example to predict in real-time the load of virus, such as the sars-cov-2, in the air by linking it to the concentration of co2.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| nonparametric bayesian inference for reversible multi-dimensional   diffusions                                                                       | we study nonparametric bayesian modelling of reversible multi-dimensional diffusions with periodic drift. for continuous observations paths, reversibility is exploited to prove a general posterior contraction rate theorem for the drift gradient vector field under approximation-theoretic conditions on the induced prior for the invariant measure. the general theorem is applied to standard gaussian priors, which are shown to converge to the truth at the minimax optimal rate over sobolev smoothness classes in any dimension. we further establish contraction rates for $p$-exponential priors over spatially inhomogeneous besov function classes for which gaussian priors are unsuitable, again in any dimension.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| learning to initialize gradient descent using gradient descent                                                                                       | non-convex optimization problems are challenging to solve; the success and computational expense of a gradient descent algorithm or variant depend heavily on the initialization strategy. often, either random initialization is used or initialization rules are carefully designed by exploiting the nature of the problem class. as a simple alternative to hand-crafted initialization rules, we propose an approach for learning "good" initialization rules from previous solutions. we provide theoretical guarantees that establish conditions that are sufficient in all cases and also necessary in some under which our approach performs better than random initialization. we apply our methodology to various non-convex problems such as generating adversarial examples, generating post hoc explanations for black-box machine learning models, and allocating communication spectrum, and show consistent gains over other initialization techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |