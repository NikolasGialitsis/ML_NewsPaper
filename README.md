<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                                               |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/deploying-a-simple-ui-for-python-88e8e7cbbf61"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/0*qq1KYo_jJmLn2hjl"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Deploying a Simple UI for Python</a></p>&nbsp;&nbsp;                                                                                                  |
| <p><a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Hacking Super Intelligence</a></p>&nbsp;&nbsp;                                                                                                    |
| <p><a href="https://towardsdatascience.com/low-effort-machine-learning-tools-9622d7d57135"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*l-_uMYUimj73v-1JlAsXKg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Low Effort Machine Learning Tools</a></p>&nbsp;&nbsp;                                                                                      |
| <p><a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Emerging problems in machine learning: making AI</a></p>&nbsp;&nbsp;                                                     |
| <p><a href="https://towardsdatascience.com/ocr-and-the-wordsearch-solver-ai-515aeb816bdf"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*5UR8Yol9sE5x0j-T2oA_UQ.gif"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">OCR and the WordSearch solver AI</a></p>&nbsp;&nbsp;                                                                                         |
| <p><a href="https://towardsdatascience.com/data-science-articles-that-provided-value-in-2020-70ba3799ed1f"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*5u8a7lENfkaasvU5AFxttA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Data Science Articles That Provided Value In 2020</a></p>&nbsp;&nbsp;                                                       |
| <p><a href="https://towardsdatascience.com/top-7-data-libraries-you-will-absolutely-need-for-your-next-deep-learning-project-ecf735f25c09"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*Y7bTsQgCcnQ25Ht-"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Top 7 Data Libraries You Will Absolutely Need for Your Next Deep Learning Project</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/machine-learning-on-macos-with-an-amd-gpu-and-plaidml-55a46fe94bc0"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*f0G4yk0STR3lDHlBxAoikg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Machine Learning on macOS with an AMD GPU and PlaidML</a></p>&nbsp;&nbsp;                                               |
| <p><a href="https://towardsdatascience.com/the-step-by-step-curriculum-im-using-to-teach-myself-data-science-in-2021-c8eab834a87c"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*oM5qx_LYK3hit2ak"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">The Step-by-Step Curriculum Using to Teach Myself Data Science in 2021</a></p>&nbsp;&nbsp;                    |
| <p><a href="https://towardsdatascience.com/5-data-science-tips-and-tricks-378517c818d6"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*Z8RkgdIFTCW7AoVlmA8PhA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">5 Data Science Tips and Tricks</a></p>&nbsp;&nbsp;                                                                                             |
| <p><a href="https://towardsdatascience.com/how-to-find-seasonality-using-python-73547ebf322"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*xDIuwKb-wRlhAifk.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">How To Find Seasonality Using Python</a></p>&nbsp;&nbsp;                                                                                        |

| title                                                                                                                                                | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| compressive sensing using iterative hard thresholding with low precision   data representation: theory and applications	doi:10.1109/tsp.2020.3010355 | modern scientific instruments produce vast amounts of data, which can overwhelm the processing ability of computer systems. lossy compression of data is an intriguing solution, but comes with its own drawbacks, such as potential signal loss, and the need for careful optimization of the compression ratio. in this work, we focus on a setting where this problem is especially acute: compressive sensing frameworks for interferometry and medical imaging. we ask the following question: can the precision of the data representation be lowered for all inputs, with recovery guarantees and practical performance? our first contribution is a theoretical analysis of the normalized iterative hard thresholding (iht) algorithm when all input data, meaning both the measurement matrix and the observation vector are quantized aggressively. we present a variant of low precision normalized {iht} that, under mild conditions, can still provide recovery guarantees. the second contribution is the application of our quantization framework to radio astronomy and magnetic resonance imaging. we show that lowering the precision of the data can significantly accelerate image recovery. we evaluate our approach on telescope data and samples of brain images using cpu and fpga implementations achieving up to a 9x speed-up with negligible loss of recovery quality.                                                                                                                                                                                                     |
| mean-field behaviour of neural tangent kernel for deep neural networks                                                                               | recent work by jacot et al. (2018) has shown that training a neural network of any kind with gradient descent in parameter space is strongly related to kernel gradient descent in function space with respect to the neural tangent kernel (ntk). lee et al. (2019) built on this result by establishing that the output of a neural network trained using gradient descent can be approximated by a linear model for wide networks. in parallel, a recent line of studies (schoenholz et al. 2017; hayou et al. 2019) has suggested that a special initialization, known as the edge of chaos, improves training. in this paper, we bridge the gap between these two concepts by quantifying the impact of the initialization and the activation function on the ntk when the network depth becomes large. in particular, we show that the performance of wide deep neural networks cannot be explained by the ntk regime and we provide experiments illustrating our theoretical results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| why do deep residual networks generalize better than deep feedforward   networks? -- a neural tangent kernel perspective                             | deep residual networks (resnets) have demonstrated better generalization performance than deep feedforward networks (ffnets). however, the theory behind such a phenomenon is still largely unknown. this paper studies this fundamental problem in deep learning from a so-called "neural tangent kernel" perspective. specifically, we first show that under proper conditions, as the width goes to infinity, training deep resnets can be viewed as learning reproducing kernel functions with some kernel function. we then compare the kernel of deep resnets with that of deep ffnets and discover that the class of functions induced by the kernel of ffnets is asymptotically not learnable, as the depth goes to infinity. in contrast, the class of functions induced by the kernel of resnets does not exhibit such degeneracy. our discovery partially justifies the advantages of deep resnets over deep ffnets in generalization abilities. numerical results are provided to support our claim.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| long-term effect estimation with surrogate representation                                                                                            | there are many scenarios where short- and long-term causal effects of an intervention are different. for example, low-quality ads may increase short-term ad clicks but decrease the long-term revenue via reduced clicks. this work, therefore, studies the problem of long-term effect where the outcome of primary interest, or primary outcome, takes months or even years to accumulate. the observational study of long-term effect presents unique challenges. first, the confounding bias causes large estimation error and variance, which can further accumulate towards the prediction of primary outcomes. second, short-term outcomes are often directly used as the proxy of the primary outcome, i.e., the surrogate. nevertheless, this method entails the strong surrogacy assumption that is often impractical. to tackle these challenges, we propose to build connections between long-term causal inference and sequential models in machine learning. this enables us to learn surrogate representations that account for the temporal unconfoundedness and circumvent the stringent surrogacy assumption by conditioning on the inferred time-varying confounders. experimental results show that the proposed framework outperforms the state-of-the-art.                                                                                                                                                                                                                                                                                                                        |
| tight bounds on the smallest eigenvalue of the neural tangent kernel for   deep relu networks                                                        | a recent line of work has analyzed the theoretical properties of deep neural networks via the neural tangent kernel (ntk). in particular, the smallest eigenvalue of the ntk has been related to memorization capacity, convergence of gradient descent algorithms and generalization of deep nets. however, existing results either provide bounds in the two-layer setting or assume that the spectrum of the ntk is bounded away from 0 for multi-layer networks. in this paper, we provide tight bounds on the smallest eigenvalue of ntk matrices for deep relu networks, both in the limiting case of infinite widths and for finite widths. in the finite-width setting, the network architectures we consider are quite general: we require the existence of a wide layer with roughly order of $n$ neurons, $n$ being the number of data samples; and the scaling of the remaining widths is arbitrary (up to logarithmic factors). to obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of feature matrices, and upper bounds on the lipschitz constant of input-output feature maps.                                                                                                                                                                                                                                                                                                                                                                                                                             |
| finding global minima via kernel approximations                                                                                                      | we consider the global minimization of smooth functions based solely on function evaluations. algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. in this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. this is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. leveraging recent representation properties of reproducing kernel hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum.   given $n$ samples, the computational cost is $o(n^{3.5})$ in time, $o(n^2)$ in space, and we achieve a convergence rate to the global optimum that is $o(n^{-m/d + 1/2 + 3/d})$ where $m$ is the degree of differentiability of the function and $d$ the number of dimensions. the rate is nearly optimal in the case of sobolev functions and more generally makes the proposed method particularly suitable for functions that have a large number of derivatives. indeed, when $m$ is in the order of $d$, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper). |
| noisy labels can induce good representations                                                                                                         | the current success of deep learning depends on large-scale labeled datasets. in practice, high-quality annotations are expensive to collect, but noisy annotations are more affordable. previous works report mixed empirical results when training with noisy labels: neural networks can easily memorize random labels, but they can also generalize from noisy labels. to explain this puzzle, we study how architecture affects learning with noisy labels. we observe that if an architecture "suits" the task, training with noisy labels can induce useful hidden representations, even when the model generalizes poorly; i.e., the last few layers of the model are more negatively affected by noisy labels. this finding leads to a simple method to improve models trained on noisy labels: replacing the final dense layers with a linear model, whose weights are learned from a small set of clean data. we empirically validate our findings across three architectures (convolutional neural networks, graph neural networks, and multi-layer perceptrons) and two domains (graph algorithmic tasks and image classification). furthermore, we achieve state-of-the-art results on image classification benchmarks by combining our method with existing approaches on noisy label training.                                                                                                                                                                                                                                                                                           |