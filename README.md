<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                                               |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/deploying-a-simple-ui-for-python-88e8e7cbbf61"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/0*qq1KYo_jJmLn2hjl"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Deploying a Simple UI for Python</a></p>&nbsp;&nbsp;                                                                                                  |
| <p><a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Hacking Super Intelligence</a></p>&nbsp;&nbsp;                                                                                                    |
| <p><a href="https://towardsdatascience.com/low-effort-machine-learning-tools-9622d7d57135"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*l-_uMYUimj73v-1JlAsXKg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Low Effort Machine Learning Tools</a></p>&nbsp;&nbsp;                                                                                      |
| <p><a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Emerging problems in machine learning: making AI</a></p>&nbsp;&nbsp;                                                     |
| <p><a href="https://towardsdatascience.com/ocr-and-the-wordsearch-solver-ai-515aeb816bdf"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*5UR8Yol9sE5x0j-T2oA_UQ.gif"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">OCR and the WordSearch solver AI</a></p>&nbsp;&nbsp;                                                                                         |
| <p><a href="https://towardsdatascience.com/data-science-articles-that-provided-value-in-2020-70ba3799ed1f"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*5u8a7lENfkaasvU5AFxttA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Data Science Articles That Provided Value In 2020</a></p>&nbsp;&nbsp;                                                       |
| <p><a href="https://towardsdatascience.com/top-7-data-libraries-you-will-absolutely-need-for-your-next-deep-learning-project-ecf735f25c09"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*Y7bTsQgCcnQ25Ht-"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Top 7 Data Libraries You Will Absolutely Need for Your Next Deep Learning Project</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/machine-learning-on-macos-with-an-amd-gpu-and-plaidml-55a46fe94bc0"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*f0G4yk0STR3lDHlBxAoikg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Machine Learning on macOS with an AMD GPU and PlaidML</a></p>&nbsp;&nbsp;                                               |
| <p><a href="https://towardsdatascience.com/the-step-by-step-curriculum-im-using-to-teach-myself-data-science-in-2021-c8eab834a87c"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*oM5qx_LYK3hit2ak"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">The Step-by-Step Curriculum Using to Teach Myself Data Science in 2021</a></p>&nbsp;&nbsp;                    |
| <p><a href="https://towardsdatascience.com/5-data-science-tips-and-tricks-378517c818d6"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*Z8RkgdIFTCW7AoVlmA8PhA.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">5 Data Science Tips and Tricks</a></p>&nbsp;&nbsp;                                                                                             |
| <p><a href="https://towardsdatascience.com/how-to-find-seasonality-using-python-73547ebf322"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*xDIuwKb-wRlhAifk.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">How To Find Seasonality Using Python</a></p>&nbsp;&nbsp;                                                                                        |

| title                                                                                                                                                | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| compressive sensing using iterative hard thresholding with low precision   data representation: theory and applications	doi:10.1109/tsp.2020.3010355 | modern scientific instruments produce vast amounts of data, which can overwhelm the processing ability of computer systems. lossy compression of data is an intriguing solution, but comes with its own drawbacks, such as potential signal loss, and the need for careful optimization of the compression ratio. in this work, we focus on a setting where this problem is especially acute: compressive sensing frameworks for interferometry and medical imaging. we ask the following question: can the precision of the data representation be lowered for all inputs, with recovery guarantees and practical performance? our first contribution is a theoretical analysis of the normalized iterative hard thresholding (iht) algorithm when all input data, meaning both the measurement matrix and the observation vector are quantized aggressively. we present a variant of low precision normalized {iht} that, under mild conditions, can still provide recovery guarantees. the second contribution is the application of our quantization framework to radio astronomy and magnetic resonance imaging. we show that lowering the precision of the data can significantly accelerate image recovery. we evaluate our approach on telescope data and samples of brain images using cpu and fpga implementations achieving up to a 9x speed-up with negligible loss of recovery quality.                                                                                                                                                                                                     |
| mean-field behaviour of neural tangent kernel for deep neural networks                                                                               | recent work by jacot et al. (2018) has shown that training a neural network of any kind with gradient descent in parameter space is strongly related to kernel gradient descent in function space with respect to the neural tangent kernel (ntk). lee et al. (2019) built on this result by establishing that the output of a neural network trained using gradient descent can be approximated by a linear model for wide networks. in parallel, a recent line of studies (schoenholz et al. 2017; hayou et al. 2019) has suggested that a special initialization, known as the edge of chaos, improves training. in this paper, we bridge the gap between these two concepts by quantifying the impact of the initialization and the activation function on the ntk when the network depth becomes large. in particular, we show that the performance of wide deep neural networks cannot be explained by the ntk regime and we provide experiments illustrating our theoretical results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| why do deep residual networks generalize better than deep feedforward   networks? -- a neural tangent kernel perspective                             | deep residual networks (resnets) have demonstrated better generalization performance than deep feedforward networks (ffnets). however, the theory behind such a phenomenon is still largely unknown. this paper studies this fundamental problem in deep learning from a so-called "neural tangent kernel" perspective. specifically, we first show that under proper conditions, as the width goes to infinity, training deep resnets can be viewed as learning reproducing kernel functions with some kernel function. we then compare the kernel of deep resnets with that of deep ffnets and discover that the class of functions induced by the kernel of ffnets is asymptotically not learnable, as the depth goes to infinity. in contrast, the class of functions induced by the kernel of resnets does not exhibit such degeneracy. our discovery partially justifies the advantages of deep resnets over deep ffnets in generalization abilities. numerical results are provided to support our claim.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| long-term effect estimation with surrogate representation                                                                                            | there are many scenarios where short- and long-term causal effects of an intervention are different. for example, low-quality ads may increase short-term ad clicks but decrease the long-term revenue via reduced clicks. this work, therefore, studies the problem of long-term effect where the outcome of primary interest, or primary outcome, takes months or even years to accumulate. the observational study of long-term effect presents unique challenges. first, the confounding bias causes large estimation error and variance, which can further accumulate towards the prediction of primary outcomes. second, short-term outcomes are often directly used as the proxy of the primary outcome, i.e., the surrogate. nevertheless, this method entails the strong surrogacy assumption that is often impractical. to tackle these challenges, we propose to build connections between long-term causal inference and sequential models in machine learning. this enables us to learn surrogate representations that account for the temporal unconfoundedness and circumvent the stringent surrogacy assumption by conditioning on the inferred time-varying confounders. experimental results show that the proposed framework outperforms the state-of-the-art.                                                                                                                                                                                                                                                                                                                        |
| tight bounds on the smallest eigenvalue of the neural tangent kernel for   deep relu networks                                                        | a recent line of work has analyzed the theoretical properties of deep neural networks via the neural tangent kernel (ntk). in particular, the smallest eigenvalue of the ntk has been related to memorization capacity, convergence of gradient descent algorithms and generalization of deep nets. however, existing results either provide bounds in the two-layer setting or assume that the spectrum of the ntk is bounded away from 0 for multi-layer networks. in this paper, we provide tight bounds on the smallest eigenvalue of ntk matrices for deep relu networks, both in the limiting case of infinite widths and for finite widths. in the finite-width setting, the network architectures we consider are quite general: we require the existence of a wide layer with roughly order of $n$ neurons, $n$ being the number of data samples; and the scaling of the remaining widths is arbitrary (up to logarithmic factors). to obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of feature matrices, and upper bounds on the lipschitz constant of input-output feature maps.                                                                                                                                                                                                                                                                                                                                                                                                                             |
| finding global minima via kernel approximations                                                                                                      | we consider the global minimization of smooth functions based solely on function evaluations. algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. in this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. this is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. leveraging recent representation properties of reproducing kernel hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum.   given $n$ samples, the computational cost is $o(n^{3.5})$ in time, $o(n^2)$ in space, and we achieve a convergence rate to the global optimum that is $o(n^{-m/d + 1/2 + 3/d})$ where $m$ is the degree of differentiability of the function and $d$ the number of dimensions. the rate is nearly optimal in the case of sobolev functions and more generally makes the proposed method particularly suitable for functions that have a large number of derivatives. indeed, when $m$ is in the order of $d$, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper). |
| noisy labels can induce good representations                                                                                                         | the current success of deep learning depends on large-scale labeled datasets. in practice, high-quality annotations are expensive to collect, but noisy annotations are more affordable. previous works report mixed empirical results when training with noisy labels: neural networks can easily memorize random labels, but they can also generalize from noisy labels. to explain this puzzle, we study how architecture affects learning with noisy labels. we observe that if an architecture "suits" the task, training with noisy labels can induce useful hidden representations, even when the model generalizes poorly; i.e., the last few layers of the model are more negatively affected by noisy labels. this finding leads to a simple method to improve models trained on noisy labels: replacing the final dense layers with a linear model, whose weights are learned from a small set of clean data. we empirically validate our findings across three architectures (convolutional neural networks, graph neural networks, and multi-layer perceptrons) and two domains (graph algorithmic tasks and image classification). furthermore, we achieve state-of-the-art results on image classification benchmarks by combining our method with existing approaches on noisy label training.                                                                                                                                                                                                                                                                                           |

| title                                                                                                                                                 | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|-------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| relationship between manifold smoothness and adversarial vulnerability   in deep learning with local errors                                           | artificial neural networks can achieve impressive performances, and even outperform humans in some specific tasks. nevertheless, unlike biological brains, the artificial neural networks suffer from tiny perturbations in sensory input, under various kinds of adversarial attacks. it is therefore necessary to study the origin of the adversarial vulnerability. here, we establish a fundamental relationship between geometry of hidden representations (manifold perspective) and the generalization capability of the deep networks. for this purpose, we choose a deep neural network trained by local errors, and then analyze emergent properties of trained networks through the manifold dimensionality, manifold smoothness, and the generalization capability. to explore effects of adversarial examples, we consider independent gaussian noise attacks and fast-gradient-sign-method (fgsm) attacks. our study reveals that a high generalization accuracy requires a relatively fast power-law decay of the eigen-spectrum of hidden representations. under gaussian attacks, the relationship between generalization accuracy and power-law exponent is monotonic, while a non-monotonic behavior is observed for fgsm attacks. our empirical study provides a route towards a final mechanistic interpretation of adversarial vulnerability under adversarial attacks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| is the brain macroscopically linear? a system identification of resting   state dynamics                                                              | a central challenge in the computational modeling of neural dynamics is the trade-off between accuracy and simplicity. at the level of individual neurons, nonlinear dynamics are both experimentally established and essential for neuronal functioning. an implicit assumption has thus formed that an accurate computational model of whole-brain dynamics must also be highly nonlinear, whereas linear models may provide a first-order approximation. here, we provide a rigorous and data-driven investigation of this hypothesis at the level of whole-brain blood-oxygen-level-dependent (bold) and macroscopic field potential dynamics by leveraging the theory of system identification. using functional mri (fmri) and intracranial eeg (ieeg), we model the resting state activity of 700 subjects in the human connectome project (hcp) and 122 subjects from the restoring active memory (ram) project using state-of-the-art linear and nonlinear model families. we assess relative model fit using predictive power, computational complexity, and the extent of residual dynamics unexplained by the model. contrary to our expectations, linear auto-regressive models achieve the best measures across all three metrics, eliminating the trade-off between accuracy and simplicity. to understand and explain this linearity, we highlight four properties of macroscopic neurodynamics which can counteract or mask microscopic nonlinear dynamics: averaging over space, averaging over time, observation noise, and limited data samples. whereas the latter two are technological limitations and can improve in the future, the former two are inherent to aggregated macroscopic brain activity. our results, together with the unparalleled interpretability of linear models, can greatly facilitate our understanding of macroscopic neural dynamics and the principled design of model-based interventions for the treatment of neuropsychiatric disorders. |
| microcircuit synchronization and heavy tailed synaptic weight   distribution in preb\"otzinger complex contribute to generation of breathing   rhythm | the preb\"otzinger complex, the mammalian inspiratory rhythm generator, encodes inspiratory time as motor pattern. spike synchronization throughout this sparsely connected network generates inspiratory bursts albeit with variable latencies after preinspiratory activity onset in each breathing cycle. using preb\"otc rhythmogenic microcircuit minimal models, we examined the variability in probability and latency to burst, mimicking experiments. among various physiologically plausible graphs of 1000 point neurons with experimentally determined neuronal and synaptic parameters, directed erd\h{o}s-r\'enyi graphs best captured the experimentally observed dynamics. mechanistically, preb\"otc (de)synchronization and oscillatory dynamics are regulated by the efferent connectivity of spiking neurons that gates the amplification of modest preinspiratory activity through input convergence. furthermore, to replicate experiments, a lognormal distribution of synaptic weights was necessary to augment the efficacy of convergent coincident inputs. these mechanisms enable exceptionally robust yet flexible preb\"otc attractor dynamics that, we postulate, represent universal temporal-processing and decision-making computational motifs throughout the brain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| the structure of behavioral data                                                                                                                      | for more than a century, scientists have been collecting behavioral data--an increasing fraction of which is now being publicly shared so other researchers can reuse them to replicate, integrate or extend past results. although behavioral data is fundamental to many scientific fields, there is currently no widely adopted standard for formatting, naming, organizing, describing or sharing such data. this lack of standardization is a major bottleneck for scientific progress. not only does it prevent the effective reuse of data, it also affects how behavioral data in general are processed, as non-standard data calls for custom-made data analysis code and prevents the development of efficient tools. to address this problem, we develop the behaverse data model (bdm), a standard for structuring behavioral data. here we focus on major concepts in behavioral data, leaving further details and developments to the project's website (https://behaverse.github.io/data-model/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |