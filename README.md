<header><h1>Your Daily Machine Learning Newspaper</h1></header>


| Towards Data Science                                                                                                                                                                                                                                                                                                                                                                                                               |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <p><a href="https://towardsdatascience.com/deploying-a-simple-ui-for-python-88e8e7cbbf61"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/1200/0*qq1KYo_jJmLn2hjl"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Deploying a Simple UI for Python</a></p>&nbsp;&nbsp;                                                                                  |
| <p><a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/1200/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Hacking Super Intelligence</a></p>&nbsp;&nbsp;                                                                                    |
| <p><a href="https://towardsdatascience.com/low-effort-machine-learning-tools-9622d7d57135"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*l-_uMYUimj73v-1JlAsXKg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Low Effort Machine Learning Tools</a></p>&nbsp;&nbsp;                                                                      |
| <p><a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Emerging problems in machine learning: making AI</a></p>&nbsp;&nbsp;                                     |
| <p><a href="https://towardsdatascience.com/ocr-and-the-wordsearch-solver-ai-515aeb816bdf"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*5UR8Yol9sE5x0j-T2oA_UQ.gif"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">OCR and the WordSearch solver AI</a></p>&nbsp;&nbsp;                                                                         |
| <p><a href="https://towardsdatascience.com/deep-learning-googlenet-explained-de8861c82765"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*k3_mlHv44pQtJ_u7D7464g.png"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Deep Learning: GoogLeNet Explained</a></p>&nbsp;&nbsp;                                                                       |
| <p><a href="https://towardsdatascience.com/classifying-images-with-feature-transformations-1fcb69b44fce"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/0*7f9_pPTauYHwJYO1"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">Classifying Images with Feature Transformations</a></p>&nbsp;&nbsp;                                                     |
| <p><a href="https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/1*QdcaMRVvRVkjDMGAhZW4Ug.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">BFGS in a Nutshell: An Introduction to Quasi-Newton Methods</a></p>&nbsp;&nbsp;                    |
| <p><a href="https://towardsdatascience.com/pycaret-and-streamlit-how-to-create-and-deploy-data-science-web-app-273d205271a3"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*m5Rq5JMxdDy-Ml7y5qeKeg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">PyCaret and Streamlit: How to Create and Deploy Data Science Web App</a></p>&nbsp;&nbsp; |
| <p><a href="https://towardsdatascience.com/what-is-tinyml-and-why-does-it-matter-f5b164766876"><img width="100" height="80" align='left' src="https://cdn-images-1.medium.com/max/800/0*YCOmGd-CMq7x7sqG"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">What is TinyML, and why does it matter?</a></p>&nbsp;&nbsp;                                                                        |
| <p><a href="https://towardsdatascience.com/6-examples-to-improve-your-sql-skills-76b40138f3cf"><img width="100" height="80" align='right' src="https://cdn-images-1.medium.com/max/800/1*v5HHnLCJIYvtZzK2wCcOjg.jpeg"                     alt="Image Missing" style="vertical-align:middle;margin:50px 0px">6 Examples to Improve Your SQL Skills</a></p>&nbsp;&nbsp;                                                              |

| title                                                                                                                                                | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| compressive sensing using iterative hard thresholding with low precision   data representation: theory and applications	doi:10.1109/tsp.2020.3010355 | modern scientific instruments produce vast amounts of data, which can overwhelm the processing ability of computer systems. lossy compression of data is an intriguing solution, but comes with its own drawbacks, such as potential signal loss, and the need for careful optimization of the compression ratio. in this work, we focus on a setting where this problem is especially acute: compressive sensing frameworks for interferometry and medical imaging. we ask the following question: can the precision of the data representation be lowered for all inputs, with recovery guarantees and practical performance? our first contribution is a theoretical analysis of the normalized iterative hard thresholding (iht) algorithm when all input data, meaning both the measurement matrix and the observation vector are quantized aggressively. we present a variant of low precision normalized {iht} that, under mild conditions, can still provide recovery guarantees. the second contribution is the application of our quantization framework to radio astronomy and magnetic resonance imaging. we show that lowering the precision of the data can significantly accelerate image recovery. we evaluate our approach on telescope data and samples of brain images using cpu and fpga implementations achieving up to a 9x speed-up with negligible loss of recovery quality.                                                                                                                                                                                                     |
| why do deep residual networks generalize better than deep feedforward   networks? -- a neural tangent kernel perspective                             | deep residual networks (resnets) have demonstrated better generalization performance than deep feedforward networks (ffnets). however, the theory behind such a phenomenon is still largely unknown. this paper studies this fundamental problem in deep learning from a so-called "neural tangent kernel" perspective. specifically, we first show that under proper conditions, as the width goes to infinity, training deep resnets can be viewed as learning reproducing kernel functions with some kernel function. we then compare the kernel of deep resnets with that of deep ffnets and discover that the class of functions induced by the kernel of ffnets is asymptotically not learnable, as the depth goes to infinity. in contrast, the class of functions induced by the kernel of resnets does not exhibit such degeneracy. our discovery partially justifies the advantages of deep resnets over deep ffnets in generalization abilities. numerical results are provided to support our claim.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| long-term effect estimation with surrogate representation                                                                                            | there are many scenarios where short- and long-term causal effects of an intervention are different. for example, low-quality ads may increase short-term ad clicks but decrease the long-term revenue via reduced clicks. this work, therefore, studies the problem of long-term effect where the outcome of primary interest, or primary outcome, takes months or even years to accumulate. the observational study of long-term effect presents unique challenges. first, the confounding bias causes large estimation error and variance, which can further accumulate towards the prediction of primary outcomes. second, short-term outcomes are often directly used as the proxy of the primary outcome, i.e., the surrogate. nevertheless, this method entails the strong surrogacy assumption that is often impractical. to tackle these challenges, we propose to build connections between long-term causal inference and sequential models in machine learning. this enables us to learn surrogate representations that account for the temporal unconfoundedness and circumvent the stringent surrogacy assumption by conditioning on the inferred time-varying confounders. experimental results show that the proposed framework outperforms the state-of-the-art.                                                                                                                                                                                                                                                                                                                        |
| tight bounds on the smallest eigenvalue of the neural tangent kernel for   deep relu networks                                                        | a recent line of work has analyzed the theoretical properties of deep neural networks via the neural tangent kernel (ntk). in particular, the smallest eigenvalue of the ntk has been related to memorization capacity, convergence of gradient descent algorithms and generalization of deep nets. however, existing results either provide bounds in the two-layer setting or assume that the spectrum of the ntk is bounded away from 0 for multi-layer networks. in this paper, we provide tight bounds on the smallest eigenvalue of ntk matrices for deep relu networks, both in the limiting case of infinite widths and for finite widths. in the finite-width setting, the network architectures we consider are quite general: we require the existence of a wide layer with roughly order of $n$ neurons, $n$ being the number of data samples; and the scaling of the remaining widths is arbitrary (up to logarithmic factors). to obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of feature matrices, and upper bounds on the lipschitz constant of input-output feature maps.                                                                                                                                                                                                                                                                                                                                                                                                                             |
| finding global minima via kernel approximations                                                                                                      | we consider the global minimization of smooth functions based solely on function evaluations. algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. in this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. this is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. leveraging recent representation properties of reproducing kernel hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum.   given $n$ samples, the computational cost is $o(n^{3.5})$ in time, $o(n^2)$ in space, and we achieve a convergence rate to the global optimum that is $o(n^{-m/d + 1/2 + 3/d})$ where $m$ is the degree of differentiability of the function and $d$ the number of dimensions. the rate is nearly optimal in the case of sobolev functions and more generally makes the proposed method particularly suitable for functions that have a large number of derivatives. indeed, when $m$ is in the order of $d$, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper). |

| title                                                                                                                                      | abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|--------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| tensor-based fusion of eeg and fmri to understand neurological changes   in schizophrenia	doi:10.1109/iscas.2017.8050303                   | neuroimaging modalities such as functional magnetic resonance imaging (fmri) and electroencephalography (eeg) provide information about neurological functions in complementary spatiotemporal resolutions; therefore, fusion of these modalities is expected to provide better understanding of brain activity. in this paper, we jointly analyze fmri and multi-channel eeg signals collected during an auditory oddball task with the goal of capturing brain activity patterns that differ between patients with schizophrenia and healthy controls. rather than selecting a single electrode or matricizing the third-order tensor that can be naturally used to represent multi-channel eeg signals, we preserve the multi-way structure of eeg data and use a coupled matrix and tensor factorization (cmtf) model to jointly analyze fmri and eeg signals. our analysis reveals that (i) joint analysis of eeg and fmri using a cmtf model can capture meaningful temporal and spatial signatures of patterns that behave differently in patients and controls, and (ii) these differences and the interpretability of the associated components increase by including multiple electrodes from frontal, motor and parietal areas, but not necessarily by including all electrodes in the analysis.                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| retrieving the structure of probabilistic sequences of auditory stimuli   from eeg data                                                    | using a new probabilistic approach we model the relationship between sequences of auditory stimuli generated by stochastic chains and the electroencephalographic (eeg) data acquired while 19 participants were exposed to those stimuli. the structure of the chains generating the stimuli are characterized by rooted and labeled trees whose leaves, henceforth called contexts, represent the sequences of past stimuli governing the choice of the next stimulus. a classical conjecture claims that the brain assigns probabilistic models to samples of stimuli. if this is true, then the context tree generating the sequence of stimuli should be encoded in the brain activity. using an innovative statistical procedure we show that this context tree can effectively be extracted from the eeg data, thus giving support to the classical conjecture.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| path-dependent connectivity, not modularity, consistently predicts   controllability of structural brain networks	doi:10.1162/netn_a_00157 | the human brain displays rich communication dynamics that are thought to be particularly well-reflected in its marked community structure. yet, the precise relationship between community structure in structural brain networks and the communication dynamics that can emerge therefrom is not well-understood. in addition to offering insight into the structure-function relationship of networked systems, such an understanding is a critical step towards the ability to manipulate the brain's large-scale dynamical activity in a targeted manner. we investigate the role of community structure in the controllability of structural brain networks. at the region level, we find that certain network measures of community structure are sometimes statistically correlated with measures of linear controllability. however, we then demonstrate that this relationship depends on the distribution of network edge weights. we highlight the complexity of the relationship between community structure and controllability by performing numerical simulations using canonical graph models with varying mesoscale architectures and edge weight distributions. finally, we demonstrate that weighted subgraph centrality, a measure rooted in the graph spectrum, and which captures higher-order graph architecture, is a stronger and more consistent predictor of controllability. our study contributes to an understanding of how the brain's diverse mesoscale structure supports transient communication dynamics.                                                                                                                                                                                                                              |
| quantum cognitive triad. semantic geometry of context representation	doi:10.1007/s10699-020-09712-x                                        | the paper describes an algorithm for semantic representation of behavioral contexts relative to a dichotomic decision alternative. the contexts are represented as quantum qubit states in two-dimensional hilbert space visualized as points on the bloch sphere. the azimuthal coordinate of this sphere functions as a one-dimensional semantic space in which the contexts are accommodated according to their subjective relevance to the considered uncertainty. the contexts are processed in triples defined by knowledge of a subject about a binary situational factor. the obtained triads of context representations function as stable cognitive structure at the same time allowing a subject to model probabilistically-variative behavior. the developed algorithm illustrates an approach for quantitative subjectively-semantic modeling of behavior based on conceptual and mathematical apparatus of quantum theory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| a deep active inference model of the rubber-hand illusion	doi:10.1007/978-3-030-64919-7_10                                                 | understanding how perception and action deal with sensorimotor conflicts, such as the rubber-hand illusion (rhi), is essential to understand how the body adapts to uncertain situations. recent results in humans have shown that the rhi not only produces a change in the perceived arm location, but also causes involuntary forces. here, we describe a deep active inference agent in a virtual environment, which we subjected to the rhi, that is able to account for these results. we show that our model, which deals with visual high-dimensional inputs, produces similar perceptual and force patterns to those found in humans.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| a new role for circuit expansion for learning in neural networks                                                                           | many sensory pathways in the brain rely on sparsely active populations of neurons downstream from the input stimuli. the biological reason for the occurrence of expanded structure in the brain is unclear, but may be because expansion can increase the expressive power of a neural network. in this work, we show that expanding a neural network can improve its generalization performance even in cases in which the expanded structure is pruned after the learning period. to study this setting we use a teacher-student framework where a perceptron teacher network generates labels which are corrupted with small amounts of noise. we then train a student network that is structurally matched to the teacher and can achieve optimal accuracy if given the teacher's synaptic weights. we find that sparse expansion of the input of a student perceptron network both increases its capacity and improves the generalization performance of the network when learning a noisy rule from a teacher perceptron when these expansions are pruned after learning. we find similar behavior when the expanded units are stochastic and uncorrelated with the input and analyze this network in the mean field limit. we show by solving the mean field equations that the generalization error of the stochastic expanded student network continues to drop as the size of the network increases. the improvement in generalization performance occurs despite the increased complexity of the student network relative to the teacher it is trying to learn. we show that this effect is closely related to the addition of slack variables in artificial neural networks and suggest possible implications for artificial and biological neural networks. |
| a geometric framework to predict structure from function in neural   networks                                                              | neural computation in biological and artificial networks relies on nonlinear synaptic integration. the structural connectivity matrix of synaptic weights between neurons is a critical determinant of overall network function, but quantitative links between neural network structure and function are complex and subtle. for example, many networks can give rise to similar functional responses, and the same network can function differently depending on context. whether certain patterns of synaptic connectivity are required to generate specific network-level computations is largely unknown. here we introduce a geometric framework for identifying synaptic connections required by steady-state responses in recurrent networks of rectified-linear neurons. assuming that the number of specified response patterns does not exceed the number of input synapses, we analytically calculate the solution space of all feedforward and recurrent connectivity matrices that can generate the specified responses from the network inputs. a generalization accounting for noise further reveals that the solution space geometry can undergo topological transitions as the allowed error increases, which could provide insight into both neuroscience and machine learning. we ultimately use this geometric characterization to derive certainty conditions guaranteeing a non-zero synapse between neurons. our theoretical framework could thus be applied to neural activity data to make rigorous anatomical predictions that follow generally from the model architecture.                                                                                                                                                                   |
| improving j-divergence of brain connectivity states by graph laplacian   denoising                                                         | functional connectivity (fc) can be represented as a network, and is frequently used to better understand the neural underpinnings of complex tasks such as motor imagery (mi) detection in brain-computer interfaces (bcis). however, errors in the estimation of connectivity can affect the detection performances. in this work, we address the problem of denoising common connectivity estimates to improve the detectability of different connectivity states. specifically, we propose a denoising algorithm that acts on the network graph laplacian, which leverages recent graph signal processing results. further, we derive a novel formulation of the jensen divergence for the denoised laplacian under different states. numerical simulations on synthetic data show that the denoising method improves the jensen divergence of connectivity patterns corresponding to different task conditions. furthermore, we apply the laplacian denoising technique to brain networks estimated from real eeg data recorded during mi-bci experiments. using our novel formulation of the j-divergence, we are able to quantify the distance between the fc networks in the motor imagery and resting states, as well as to understand the contribution of each laplacian variable to the total j-divergence between two states. experimental results on real mi-bci eeg data demonstrate that the laplacian denoising improves the separation of motor imagery and resting mental states, and shortens the time interval required for connectivity estimation. we conclude that the approach shows promise for the robust detection of connectivity states while being appealing for implementation in real-time bci applications.                              |